# -*- coding: utf-8 -*-
"""Identification of proper noun

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16WJVkQ7yGEtLyI8XTpxYqG4bqJkKIW1y
"""

! pip install transformers evaluate

import transformers

print(transformers.__version__)

"""# GPU"""

import torch

# Confirm that the GPU is detected

assert torch.cuda.is_available()

# Get the GPU device name.
device_name = torch.cuda.get_device_name()
n_gpu = torch.cuda.device_count()
print(f"Found device: {device_name}, n_gpu: {n_gpu}")
device = torch.device("cuda")

"""# Fine-tuning a model on a token classification task"""

task = "proper_noun" # Should be one of "ner", "pos" or "chunk"
model_checkpoint = "bert-base-uncased"
batch_size = 16
model_name = "bert-base-uncased"

"""## Loading the dataset"""

from google.colab import drive
import pandas as pd
drive.mount('/content/drive')

# df =pd.read_json("drive/My\ Drive/Sem4/696DS/Notebooks/collective_noun.json", orient='split')
# df.head()
import json
with open('drive/My Drive/Sem4/696DS/Notebooks/proper_noun.json', encoding='utf-8-sig') as fp:
    data = json.loads(''.join(line.strip() for line in fp))
df1 = pd.DataFrame(data["with_proper_noun"])
df1["Label"] = 1
df2 = pd.DataFrame(data["without_proper_noun"])
df2["Label"] = 0

df = pd.concat([df1, df2])
df

df = df.rename(columns={0: 'Sentence'})
df

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df["Sentence"], df["Label"], test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)

"""## Preprocessing the data"""

from transformers import AutoTokenizer
    
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

import transformers
assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)

train_encodings = tokenizer(X_train.values.tolist(), truncation=True, padding=True)
val_encodings = tokenizer(X_val.values.tolist(), truncation=True, padding=True)
test_encodings = tokenizer(X_test.values.tolist(), truncation=True, padding=True)

import torch

class PNDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = PNDataset(train_encodings, y_train.values.tolist())
val_dataset = PNDataset(val_encodings, y_val.values.tolist())
test_dataset = PNDataset(test_encodings, y_test.values.tolist())

# train_dataset[0]

"""## Fine-tuning the model"""

import evaluate

accuracy = evaluate.load("accuracy")

import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=labels)

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased", num_labels=2
)

from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

training_args = TrainingArguments(
    output_dir="proper_noun",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

trainer.save_model('{model_name}-finetuned-{task}')



from scipy.stats import ttest_ind
import numpy as np
# import matplotlib.pyplot as plt
# import ipdb
class NeuronAnalyzer: 
    
    '''
    Take cls embeddings from pretrained and finetuned models and perform statistical analysis on the neurons.
    '''
    
    def __init__(self, pretained_activations, finetuned_activations) -> None:
        self.pretained_activations = pretained_activations
        self.finetuned_activations = finetuned_activations
        
        self.neuron_rankings = None
        
    
    def plot_neuron_histograms(self, top_k=10, save_path="./figs"):
        neuron_indices = self.rank_neuron()[:top_k]
        for top_i, neuron_idx in enumerate(neuron_indices): 
            
            plt.hist(self.pretained_activations[:, neuron_idx], bins=20, alpha=0.5, label="pretrained", color='blue')
            plt.hist(self.finetuned_activations[:, neuron_idx], bins=20, alpha=0.5, label="finetuned", color='red')
            plt.title('Histogram of activations for neuron {}'.format(neuron_idx))
            plt.xlabel('Actviations')
            plt.ylabel('Frequency')
            plt.legend(loc='upper right')
            plt.savefig(f"{save_path}/histograms_top{top_i}_neuron_{neuron_idx}.png")
            plt.clf()

    # def compute_spearman_stat(self):
    #     self.neuron_rankings = []
    #     self._rank_neuron()
    
    def rank_neuron(self, method="t_stat"): 
        if method == "t_stat":
            t_stats = self._compute_t_stat()
            # Get the indices of the sorted array in descending order
            sorted_neurons_indices = np.argsort(t_stats)[::-1]
        
        return list(sorted_neurons_indices)
        
    def _compute_t_stat(self, alpha=0.05):
        # num_of_neurons = self.pretained_activations.shape[1]
        t_stats, p_vals = ttest_ind(self.pretained_activations, self.finetuned_activations, axis=0)
        significant_neurons = np.where(p_vals < alpha)[0]
        # ipdb.set_trace()
        return t_stats[significant_neurons]

class NeuronExtractor: 
    
    '''
    Take a trained model (either pretrained or finetuned) and N test sentences and return the cls embedding of shape (N, d_model). 
    '''
    
    def __init__(self, model, tokenizer, is_split_into_words=False) -> None:
        self.model = model
        self.tokenizer = tokenizer
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.is_split_into_words = is_split_into_words
        self.model.to(self.device)
    
    # todo: batched inference, due to the limited memory of the GPU
    @torch.no_grad()
    def extract_cls(self, sentences):
        input_ids = self.tokenizer(sentences, padding=True, truncation=True, return_tensors="pt", is_split_into_words=self.is_split_into_words).to(self.device)
        outputs = self.model(**input_ids)
        cls_embedding = outputs.last_hidden_state[:, 0, :].detach().cpu().numpy()
        return cls_embedding
    
    @torch.no_grad()
    def extract_layer(self, sentences, layer_num):
        input_ids = self.tokenizer(sentences, padding=True, truncation=True, return_tensors="pt", is_split_into_words=self.is_split_into_words).to(self.device)
        outputs = self.model(**input_ids)
        embedding = outputs.hidden_states[layer_num-1].detach().cpu().numpy()
        return embedding

import torch
import transformers
from transformers import AutoModel, AutoTokenizer
# dataset_name1 = "ptb_text_only"
# dataset1 = load_dataset(dataset_name1, split="test").select(range(500))
layer_embs_pretrained = []

# Load the pretrained model
model_checkpoint = "bert-base-uncased"
model_pretrained = AutoModel.from_pretrained(model_checkpoint, output_hidden_states=True)
tokenizer_pretrained = AutoTokenizer.from_pretrained(model_checkpoint)
cls_emb_pretrained = NeuronExtractor(model_pretrained, tokenizer_pretrained, is_split_into_words=True).extract_cls(X_test.values.tolist())
for layer_num in range(1,13):

  layer_emb_pretrained = NeuronExtractor(model_pretrained, tokenizer_pretrained, is_split_into_words=True).extract_layer(X_test.values.tolist(), layer_num)
  layer_embs_pretrained.append(layer_emb_pretrained)
# cls_emb_pretrained = NeuronExtractor(model_pretrained, tokenizer_pretrained).extract_cls(dataset["sentence"])
# layer1_emb_pretrained = NeuronExtractor(model_pretrained, tokenizer_pretrained).extract_layer(dataset1["sentence"], 2)

# Load the finetuned model
model_checkpoint_finetuned = "{model_name}-finetuned-{task}"
model_finetuned = AutoModel.from_pretrained(model_checkpoint_finetuned, output_hidden_states=True)
tokenizer_finetuned = AutoTokenizer.from_pretrained(model_checkpoint)
layer_embs_finetuned = []
cls_emb_finetuned = NeuronExtractor(model_finetuned, tokenizer_finetuned, is_split_into_words=True).extract_cls(X_test.values.tolist())
for layer_num in range(1,13):

  layer_emb_finetuned = NeuronExtractor(model_finetuned, tokenizer_finetuned, is_split_into_words=True).extract_layer(X_test.values.tolist(), layer_num)
  layer_embs_finetuned.append(layer_emb_finetuned)
# cls_emb_finetuned = NeuronExtractor(model_finetuned, tokenizer_finetuned).extract_cls(dataset1["sentence"])
# layer1_emb_finetuned = NeuronExtractor(model_finetuned, tokenizer_finetuned).extract_layer(dataset1["sentence"], 2)

print(layer_embs_pretrained[1].shape)
# print(layer1_emb_finetuned.shape)

for i in range(len(layer_embs_pretrained)):
  layer_embs_pretrained[i] = np.mean(layer_embs_pretrained[i], axis = 0)
  layer_embs_pretrained[i] = np.mean(layer_embs_pretrained[i], axis = 0)
  print(layer_embs_pretrained[i].shape)

for i in range(len(layer_embs_finetuned)):
  layer_embs_finetuned[i] = np.mean(layer_embs_finetuned[i], axis = 0)
  layer_embs_finetuned[i] = np.mean(layer_embs_finetuned[i], axis = 0)
  print(layer_embs_finetuned[i].shape)

import seaborn as sns
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate some example data
for i in range(len(layer_embs_finetuned)):
  lis = [layer_embs_pretrained[i], layer_embs_finetuned[i]]
  # print(data.shape)
  data = pd.DataFrame(lis)

  # Create the heatmap using seaborn
  sns.heatmap(data)

  # Show the plot
  plt.title("Layer " + str(i+1) + " activations")
  plt.show()

# sns.heatmap(cls_emb_finetuned)
# plt.show()

