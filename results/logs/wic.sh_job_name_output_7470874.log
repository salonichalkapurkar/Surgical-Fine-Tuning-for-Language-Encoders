Loading miniconda version 22.11.1-1
######################################################################
layer-wise fine-tuning top 1
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 333.01it/s]
/home/yuanmingtao_umass_edu/.conda/envs/petl/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.4', 'bert.encoder.layer.5', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 0.7462112903594971, 'eval_accuracy': 0.493730407523511, 'eval_runtime': 20.8474, 'eval_samples_per_second': 30.603, 'eval_steps_per_second': 3.837, 'epoch': 1.0}
{'loss': 0.7796, 'learning_rate': 2.5e-06, 'epoch': 1.47}
{'eval_loss': 0.6666042804718018, 'eval_accuracy': 0.6050156739811913, 'eval_runtime': 20.7977, 'eval_samples_per_second': 30.677, 'eval_steps_per_second': 3.847, 'epoch': 2.0}
{'loss': 0.6583, 'learning_rate': 5e-06, 'epoch': 2.95}
{'eval_loss': 0.6530818343162537, 'eval_accuracy': 0.6410658307210031, 'eval_runtime': 20.7917, 'eval_samples_per_second': 30.685, 'eval_steps_per_second': 3.848, 'epoch': 3.0}
{'eval_loss': 0.6531369090080261, 'eval_accuracy': 0.6379310344827587, 'eval_runtime': 20.7834, 'eval_samples_per_second': 30.698, 'eval_steps_per_second': 3.849, 'epoch': 4.0}
{'loss': 0.6257, 'learning_rate': 7.5e-06, 'epoch': 4.42}
{'eval_loss': 0.6530277729034424, 'eval_accuracy': 0.6520376175548589, 'eval_runtime': 20.7858, 'eval_samples_per_second': 30.694, 'eval_steps_per_second': 3.849, 'epoch': 5.0}
{'loss': 0.582, 'learning_rate': 1e-05, 'epoch': 5.89}
{'eval_loss': 0.6593173742294312, 'eval_accuracy': 0.6473354231974922, 'eval_runtime': 20.8499, 'eval_samples_per_second': 30.6, 'eval_steps_per_second': 3.837, 'epoch': 6.0}
{'eval_loss': 0.6946084499359131, 'eval_accuracy': 0.6442006269592476, 'eval_runtime': 20.8074, 'eval_samples_per_second': 30.662, 'eval_steps_per_second': 3.845, 'epoch': 7.0}
{'loss': 0.5206, 'learning_rate': 1.25e-05, 'epoch': 7.36}
{'eval_loss': 0.717382550239563, 'eval_accuracy': 0.6473354231974922, 'eval_runtime': 20.8337, 'eval_samples_per_second': 30.624, 'eval_steps_per_second': 3.84, 'epoch': 8.0}
{'loss': 0.4499, 'learning_rate': 1.5e-05, 'epoch': 8.84}
{'eval_loss': 0.7690528035163879, 'eval_accuracy': 0.6473354231974922, 'eval_runtime': 20.7981, 'eval_samples_per_second': 30.676, 'eval_steps_per_second': 3.847, 'epoch': 9.0}
{'eval_loss': 0.8346198201179504, 'eval_accuracy': 0.6504702194357367, 'eval_runtime': 20.7758, 'eval_samples_per_second': 30.709, 'eval_steps_per_second': 3.851, 'epoch': 9.99}
{'train_runtime': 4198.5991, 'train_samples_per_second': 12.928, 'train_steps_per_second': 0.807, 'train_loss': 0.5744691201719211, 'epoch': 9.99}
Total training time: 4198.61 seconds
######################################################################
layer-wise fine-tuning top 2
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 336.35it/s]
/home/yuanmingtao_umass_edu/.conda/envs/petl/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.2', 'bert.encoder.layer.4', 'bert.encoder.layer.5', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 0.742647111415863, 'eval_accuracy': 0.493730407523511, 'eval_runtime': 20.7319, 'eval_samples_per_second': 30.774, 'eval_steps_per_second': 3.859, 'epoch': 1.0}
{'loss': 0.7773, 'learning_rate': 2.5e-06, 'epoch': 1.47}
{'eval_loss': 0.6613420248031616, 'eval_accuracy': 0.6144200626959248, 'eval_runtime': 20.7376, 'eval_samples_per_second': 30.765, 'eval_steps_per_second': 3.858, 'epoch': 2.0}
{'loss': 0.651, 'learning_rate': 5e-06, 'epoch': 2.95}
{'eval_loss': 0.6469148993492126, 'eval_accuracy': 0.658307210031348, 'eval_runtime': 20.7636, 'eval_samples_per_second': 30.727, 'eval_steps_per_second': 3.853, 'epoch': 3.0}
{'eval_loss': 0.6531182527542114, 'eval_accuracy': 0.64576802507837, 'eval_runtime': 20.7282, 'eval_samples_per_second': 30.779, 'eval_steps_per_second': 3.859, 'epoch': 4.0}
{'loss': 0.5994, 'learning_rate': 7.5e-06, 'epoch': 4.42}
{'eval_loss': 0.6651198863983154, 'eval_accuracy': 0.6520376175548589, 'eval_runtime': 20.8136, 'eval_samples_per_second': 30.653, 'eval_steps_per_second': 3.844, 'epoch': 5.0}
{'loss': 0.5194, 'learning_rate': 1e-05, 'epoch': 5.89}
{'eval_loss': 0.675445556640625, 'eval_accuracy': 0.6661442006269592, 'eval_runtime': 20.7457, 'eval_samples_per_second': 30.753, 'eval_steps_per_second': 3.856, 'epoch': 6.0}
{'eval_loss': 0.7366980314254761, 'eval_accuracy': 0.6551724137931034, 'eval_runtime': 20.7239, 'eval_samples_per_second': 30.786, 'eval_steps_per_second': 3.86, 'epoch': 7.0}
{'loss': 0.4119, 'learning_rate': 1.25e-05, 'epoch': 7.36}
{'eval_loss': 0.783848226070404, 'eval_accuracy': 0.6504702194357367, 'eval_runtime': 20.7829, 'eval_samples_per_second': 30.698, 'eval_steps_per_second': 3.849, 'epoch': 8.0}
{'loss': 0.3084, 'learning_rate': 1.5e-05, 'epoch': 8.84}
{'eval_loss': 0.9144557118415833, 'eval_accuracy': 0.6520376175548589, 'eval_runtime': 20.7478, 'eval_samples_per_second': 30.75, 'eval_steps_per_second': 3.856, 'epoch': 9.0}
{'eval_loss': 1.023797631263733, 'eval_accuracy': 0.6551724137931034, 'eval_runtime': 20.7454, 'eval_samples_per_second': 30.754, 'eval_steps_per_second': 3.856, 'epoch': 9.99}
{'train_runtime': 4308.5916, 'train_samples_per_second': 12.598, 'train_steps_per_second': 0.787, 'train_loss': 0.5050214604290537, 'epoch': 9.99}
Total training time: 4308.61 seconds
######################################################################
layer-wise fine-tuning top 3
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 222.45it/s]
/home/yuanmingtao_umass_edu/.conda/envs/petl/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.2', 'bert.encoder.layer.4', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 0.735715389251709, 'eval_accuracy': 0.49216300940438873, 'eval_runtime': 20.8387, 'eval_samples_per_second': 30.616, 'eval_steps_per_second': 3.839, 'epoch': 1.0}
{'loss': 0.7736, 'learning_rate': 2.5e-06, 'epoch': 1.47}
{'eval_loss': 0.6512876152992249, 'eval_accuracy': 0.64576802507837, 'eval_runtime': 20.753, 'eval_samples_per_second': 30.742, 'eval_steps_per_second': 3.855, 'epoch': 2.0}
{'loss': 0.6387, 'learning_rate': 5e-06, 'epoch': 2.95}
{'eval_loss': 0.6412606835365295, 'eval_accuracy': 0.658307210031348, 'eval_runtime': 20.7639, 'eval_samples_per_second': 30.726, 'eval_steps_per_second': 3.853, 'epoch': 3.0}
{'eval_loss': 0.6628744006156921, 'eval_accuracy': 0.6504702194357367, 'eval_runtime': 20.8155, 'eval_samples_per_second': 30.65, 'eval_steps_per_second': 3.843, 'epoch': 4.0}
{'loss': 0.5549, 'learning_rate': 7.5e-06, 'epoch': 4.42}
{'eval_loss': 0.6886814832687378, 'eval_accuracy': 0.6739811912225705, 'eval_runtime': 20.8711, 'eval_samples_per_second': 30.569, 'eval_steps_per_second': 3.833, 'epoch': 5.0}
{'loss': 0.4421, 'learning_rate': 1e-05, 'epoch': 5.89}
{'eval_loss': 0.7181527614593506, 'eval_accuracy': 0.6724137931034483, 'eval_runtime': 20.7471, 'eval_samples_per_second': 30.751, 'eval_steps_per_second': 3.856, 'epoch': 6.0}
{'eval_loss': 0.8138655424118042, 'eval_accuracy': 0.6818181818181818, 'eval_runtime': 20.7727, 'eval_samples_per_second': 30.713, 'eval_steps_per_second': 3.851, 'epoch': 7.0}
{'loss': 0.2951, 'learning_rate': 1.25e-05, 'epoch': 7.36}
{'eval_loss': 0.9167491793632507, 'eval_accuracy': 0.6865203761755486, 'eval_runtime': 20.7729, 'eval_samples_per_second': 30.713, 'eval_steps_per_second': 3.851, 'epoch': 8.0}
{'loss': 0.1878, 'learning_rate': 1.5e-05, 'epoch': 8.84}
{'eval_loss': 1.1095552444458008, 'eval_accuracy': 0.670846394984326, 'eval_runtime': 20.7571, 'eval_samples_per_second': 30.736, 'eval_steps_per_second': 3.854, 'epoch': 9.0}
{'eval_loss': 1.3944491147994995, 'eval_accuracy': 0.6692789968652038, 'eval_runtime': 20.8213, 'eval_samples_per_second': 30.642, 'eval_steps_per_second': 3.842, 'epoch': 9.99}
{'train_runtime': 4421.0006, 'train_samples_per_second': 12.278, 'train_steps_per_second': 0.767, 'train_loss': 0.4383327011513499, 'epoch': 9.99}
Total training time: 4421.02 seconds
######################################################################
layer-wise fine-tuning top 4
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 371.53it/s]
/home/yuanmingtao_umass_edu/.conda/envs/petl/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.2', 'bert.encoder.layer.4', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 0.7337592840194702, 'eval_accuracy': 0.49216300940438873, 'eval_runtime': 20.7476, 'eval_samples_per_second': 30.751, 'eval_steps_per_second': 3.856, 'epoch': 1.0}
{'loss': 0.7724, 'learning_rate': 2.5e-06, 'epoch': 1.47}
{'eval_loss': 0.6489365696907043, 'eval_accuracy': 0.6410658307210031, 'eval_runtime': 20.789, 'eval_samples_per_second': 30.689, 'eval_steps_per_second': 3.848, 'epoch': 2.0}
{'loss': 0.6349, 'learning_rate': 5e-06, 'epoch': 2.95}
{'eval_loss': 0.636684238910675, 'eval_accuracy': 0.6536050156739812, 'eval_runtime': 20.7284, 'eval_samples_per_second': 30.779, 'eval_steps_per_second': 3.859, 'epoch': 3.0}
{'eval_loss': 0.6643932461738586, 'eval_accuracy': 0.6677115987460815, 'eval_runtime': 20.8224, 'eval_samples_per_second': 30.64, 'eval_steps_per_second': 3.842, 'epoch': 4.0}
{'loss': 0.5372, 'learning_rate': 7.5e-06, 'epoch': 4.42}
{'eval_loss': 0.6951018571853638, 'eval_accuracy': 0.6692789968652038, 'eval_runtime': 20.7224, 'eval_samples_per_second': 30.788, 'eval_steps_per_second': 3.861, 'epoch': 5.0}
{'loss': 0.4053, 'learning_rate': 1e-05, 'epoch': 5.89}
{'eval_loss': 0.7389830350875854, 'eval_accuracy': 0.6755485893416928, 'eval_runtime': 20.8223, 'eval_samples_per_second': 30.64, 'eval_steps_per_second': 3.842, 'epoch': 6.0}
{'eval_loss': 0.8741780519485474, 'eval_accuracy': 0.6724137931034483, 'eval_runtime': 20.8055, 'eval_samples_per_second': 30.665, 'eval_steps_per_second': 3.845, 'epoch': 7.0}
{'loss': 0.2503, 'learning_rate': 1.25e-05, 'epoch': 7.36}
{'eval_loss': 0.9818886518478394, 'eval_accuracy': 0.6802507836990596, 'eval_runtime': 20.7709, 'eval_samples_per_second': 30.716, 'eval_steps_per_second': 3.852, 'epoch': 8.0}
{'loss': 0.1508, 'learning_rate': 1.5e-05, 'epoch': 8.84}
{'eval_loss': 1.217047095298767, 'eval_accuracy': 0.6724137931034483, 'eval_runtime': 20.7321, 'eval_samples_per_second': 30.774, 'eval_steps_per_second': 3.859, 'epoch': 9.0}
{'eval_loss': 1.4711445569992065, 'eval_accuracy': 0.6677115987460815, 'eval_runtime': 20.7204, 'eval_samples_per_second': 30.791, 'eval_steps_per_second': 3.861, 'epoch': 9.99}
{'train_runtime': 4535.7897, 'train_samples_per_second': 11.967, 'train_steps_per_second': 0.747, 'train_loss': 0.4150519958991217, 'epoch': 9.99}
Total training time: 4535.80 seconds
######################################################################
layer-wise fine-tuning top 5
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 192.97it/s]
/home/yuanmingtao_umass_edu/.conda/envs/petl/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.2', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 0.7278693914413452, 'eval_accuracy': 0.49216300940438873, 'eval_runtime': 20.6797, 'eval_samples_per_second': 30.851, 'eval_steps_per_second': 3.869, 'epoch': 1.0}
{'loss': 0.7692, 'learning_rate': 2.5e-06, 'epoch': 1.47}
{'eval_loss': 0.6464976072311401, 'eval_accuracy': 0.6473354231974922, 'eval_runtime': 20.9452, 'eval_samples_per_second': 30.46, 'eval_steps_per_second': 3.819, 'epoch': 2.0}
{'loss': 0.6243, 'learning_rate': 5e-06, 'epoch': 2.95}
{'eval_loss': 0.636372983455658, 'eval_accuracy': 0.6551724137931034, 'eval_runtime': 20.9218, 'eval_samples_per_second': 30.495, 'eval_steps_per_second': 3.824, 'epoch': 3.0}
{'eval_loss': 0.6671097874641418, 'eval_accuracy': 0.6692789968652038, 'eval_runtime': 20.7198, 'eval_samples_per_second': 30.792, 'eval_steps_per_second': 3.861, 'epoch': 4.0}
{'loss': 0.5009, 'learning_rate': 7.5e-06, 'epoch': 4.42}
{'eval_loss': 0.7054368257522583, 'eval_accuracy': 0.6818181818181818, 'eval_runtime': 20.7111, 'eval_samples_per_second': 30.805, 'eval_steps_per_second': 3.863, 'epoch': 5.0}
{'loss': 0.3458, 'learning_rate': 1e-05, 'epoch': 5.89}
{'eval_loss': 0.7918789386749268, 'eval_accuracy': 0.6833855799373041, 'eval_runtime': 20.7141, 'eval_samples_per_second': 30.8, 'eval_steps_per_second': 3.862, 'epoch': 6.0}
{'eval_loss': 0.9866691827774048, 'eval_accuracy': 0.6739811912225705, 'eval_runtime': 20.7377, 'eval_samples_per_second': 30.765, 'eval_steps_per_second': 3.858, 'epoch': 7.0}
{'loss': 0.1849, 'learning_rate': 1.25e-05, 'epoch': 7.36}
{'eval_loss': 1.2064656019210815, 'eval_accuracy': 0.670846394984326, 'eval_runtime': 20.8651, 'eval_samples_per_second': 30.577, 'eval_steps_per_second': 3.834, 'epoch': 8.0}
{'loss': 0.11, 'learning_rate': 1.5e-05, 'epoch': 8.84}
{'eval_loss': 1.4309989213943481, 'eval_accuracy': 0.670846394984326, 'eval_runtime': 20.8915, 'eval_samples_per_second': 30.539, 'eval_steps_per_second': 3.829, 'epoch': 9.0}
{'eval_loss': 1.590278148651123, 'eval_accuracy': 0.6598746081504702, 'eval_runtime': 20.7953, 'eval_samples_per_second': 30.68, 'eval_steps_per_second': 3.847, 'epoch': 9.99}
{'train_runtime': 4646.2144, 'train_samples_per_second': 11.683, 'train_steps_per_second': 0.73, 'train_loss': 0.3813402929840538, 'epoch': 9.99}
Total training time: 4646.23 seconds
######################################################################
finished
