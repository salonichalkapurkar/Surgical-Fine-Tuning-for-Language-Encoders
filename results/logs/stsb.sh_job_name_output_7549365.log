######################################################################
full model fine-tuning
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 30.57it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'eval_loss': 3.1661012172698975, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.8037, 'eval_samples_per_second': 28.407, 'eval_steps_per_second': 3.56, 'epoch': 1.0}
{'loss': 4.4041, 'learning_rate': 2.5e-06, 'epoch': 1.39}
{'eval_loss': 0.6740052700042725, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.7381, 'eval_samples_per_second': 28.442, 'eval_steps_per_second': 3.565, 'epoch': 2.0}
{'loss': 0.7535, 'learning_rate': 5e-06, 'epoch': 2.78}
{'eval_loss': 0.589287519454956, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6865, 'eval_samples_per_second': 28.47, 'eval_steps_per_second': 3.568, 'epoch': 3.0}
{'eval_loss': 0.627828061580658, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 53.0632, 'eval_samples_per_second': 28.268, 'eval_steps_per_second': 3.543, 'epoch': 4.0}
{'loss': 0.4963, 'learning_rate': 7.5e-06, 'epoch': 4.17}
{'eval_loss': 0.4673771262168884, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.7195, 'eval_samples_per_second': 28.452, 'eval_steps_per_second': 3.566, 'epoch': 5.0}
{'loss': 0.3246, 'learning_rate': 1e-05, 'epoch': 5.56}
{'eval_loss': 0.47118425369262695, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6448, 'eval_samples_per_second': 28.493, 'eval_steps_per_second': 3.571, 'epoch': 6.0}
{'loss': 0.2271, 'learning_rate': 1.25e-05, 'epoch': 6.95}
{'eval_loss': 0.5003071427345276, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.7055, 'eval_samples_per_second': 28.46, 'eval_steps_per_second': 3.567, 'epoch': 7.0}
{'eval_loss': 0.47661709785461426, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 53.4537, 'eval_samples_per_second': 28.062, 'eval_steps_per_second': 3.517, 'epoch': 8.0}
{'loss': 0.1612, 'learning_rate': 1.5e-05, 'epoch': 8.34}
{'eval_loss': 0.4796113967895508, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 53.5202, 'eval_samples_per_second': 28.027, 'eval_steps_per_second': 3.513, 'epoch': 9.0}
{'loss': 0.1404, 'learning_rate': 1.75e-05, 'epoch': 9.74}
{'eval_loss': 0.48669376969337463, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6578, 'eval_samples_per_second': 28.486, 'eval_steps_per_second': 3.57, 'epoch': 9.99}
{'train_runtime': 6322.542, 'train_samples_per_second': 9.093, 'train_steps_per_second': 0.568, 'train_loss': 0.9093450110602844, 'epoch': 9.99}
Total training time: 6322.58 seconds
######################################################################
layer-wise fine-tuning top 1
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:00<00:00,  4.28it/s] 67%|██████▋   | 2/3 [00:00<00:00,  3.10it/s]100%|██████████| 3/3 [00:00<00:00,  4.47it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.1', 'bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.4', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 3.8619022369384766, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6084, 'eval_samples_per_second': 28.513, 'eval_steps_per_second': 3.574, 'epoch': 1.0}
{'loss': 5.3295, 'learning_rate': 2.5e-06, 'epoch': 1.39}
{'eval_loss': 1.8209222555160522, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 53.2543, 'eval_samples_per_second': 28.167, 'eval_steps_per_second': 3.53, 'epoch': 2.0}
{'loss': 2.1663, 'learning_rate': 5e-06, 'epoch': 2.78}
{'eval_loss': 0.919974684715271, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.5857, 'eval_samples_per_second': 28.525, 'eval_steps_per_second': 3.575, 'epoch': 3.0}
{'eval_loss': 0.7458701133728027, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6475, 'eval_samples_per_second': 28.491, 'eval_steps_per_second': 3.571, 'epoch': 4.0}
{'loss': 0.9568, 'learning_rate': 7.5e-06, 'epoch': 4.17}
{'eval_loss': 0.6588722467422485, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6925, 'eval_samples_per_second': 28.467, 'eval_steps_per_second': 3.568, 'epoch': 5.0}
{'loss': 0.745, 'learning_rate': 1e-05, 'epoch': 5.56}
{'eval_loss': 0.6037446856498718, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6478, 'eval_samples_per_second': 28.491, 'eval_steps_per_second': 3.571, 'epoch': 6.0}
{'loss': 0.5941, 'learning_rate': 1.25e-05, 'epoch': 6.95}
{'eval_loss': 0.5895807147026062, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.651, 'eval_samples_per_second': 28.49, 'eval_steps_per_second': 3.571, 'epoch': 7.0}
{'eval_loss': 0.5714678764343262, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.5904, 'eval_samples_per_second': 28.522, 'eval_steps_per_second': 3.575, 'epoch': 8.0}
{'loss': 0.4614, 'learning_rate': 1.5e-05, 'epoch': 8.34}
{'eval_loss': 0.5584475994110107, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6644, 'eval_samples_per_second': 28.482, 'eval_steps_per_second': 3.57, 'epoch': 9.0}
{'loss': 0.3772, 'learning_rate': 1.75e-05, 'epoch': 9.74}
{'eval_loss': 0.5697927474975586, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6486, 'eval_samples_per_second': 28.491, 'eval_steps_per_second': 3.571, 'epoch': 9.99}
{'train_runtime': 4983.0626, 'train_samples_per_second': 11.537, 'train_steps_per_second': 0.72, 'train_loss': 1.4891382812457497, 'epoch': 9.99}
Total training time: 4983.09 seconds
######################################################################
layer-wise fine-tuning top 2
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 58.21it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.4', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 3.855280637741089, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6617, 'eval_samples_per_second': 28.484, 'eval_steps_per_second': 3.57, 'epoch': 1.0}
{'loss': 5.3215, 'learning_rate': 2.5e-06, 'epoch': 1.39}
{'eval_loss': 1.8052700757980347, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.5963, 'eval_samples_per_second': 28.519, 'eval_steps_per_second': 3.574, 'epoch': 2.0}
{'loss': 2.1418, 'learning_rate': 5e-06, 'epoch': 2.78}
{'eval_loss': 0.8970844745635986, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6206, 'eval_samples_per_second': 28.506, 'eval_steps_per_second': 3.573, 'epoch': 3.0}
{'eval_loss': 0.6973071098327637, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6292, 'eval_samples_per_second': 28.501, 'eval_steps_per_second': 3.572, 'epoch': 4.0}
{'loss': 0.8987, 'learning_rate': 7.5e-06, 'epoch': 4.17}
{'eval_loss': 0.6249801516532898, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.5854, 'eval_samples_per_second': 28.525, 'eval_steps_per_second': 3.575, 'epoch': 5.0}
{'loss': 0.6676, 'learning_rate': 1e-05, 'epoch': 5.56}
{'eval_loss': 0.5904811024665833, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.5931, 'eval_samples_per_second': 28.521, 'eval_steps_per_second': 3.575, 'epoch': 6.0}
{'loss': 0.5163, 'learning_rate': 1.25e-05, 'epoch': 6.95}
{'eval_loss': 0.5707074403762817, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.5866, 'eval_samples_per_second': 28.524, 'eval_steps_per_second': 3.575, 'epoch': 7.0}
{'eval_loss': 0.5685792565345764, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6207, 'eval_samples_per_second': 28.506, 'eval_steps_per_second': 3.573, 'epoch': 8.0}
{'loss': 0.3792, 'learning_rate': 1.5e-05, 'epoch': 8.34}
{'eval_loss': 0.563836932182312, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6598, 'eval_samples_per_second': 28.485, 'eval_steps_per_second': 3.57, 'epoch': 9.0}
{'loss': 0.2986, 'learning_rate': 1.75e-05, 'epoch': 9.74}
{'eval_loss': 0.5633239150047302, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6708, 'eval_samples_per_second': 28.479, 'eval_steps_per_second': 3.569, 'epoch': 9.99}
{'train_runtime': 5098.5599, 'train_samples_per_second': 11.276, 'train_steps_per_second': 0.704, 'train_loss': 1.430503986007988, 'epoch': 9.99}
Total training time: 5098.58 seconds
######################################################################
layer-wise fine-tuning top 3
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 48.84it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 3.833824634552002, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.689, 'eval_samples_per_second': 28.469, 'eval_steps_per_second': 3.568, 'epoch': 1.0}
{'loss': 5.3007, 'learning_rate': 2.5e-06, 'epoch': 1.39}
{'eval_loss': 1.7819161415100098, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.617, 'eval_samples_per_second': 28.508, 'eval_steps_per_second': 3.573, 'epoch': 2.0}
{'loss': 2.0945, 'learning_rate': 5e-06, 'epoch': 2.78}
{'eval_loss': 0.7427722215652466, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6665, 'eval_samples_per_second': 28.481, 'eval_steps_per_second': 3.57, 'epoch': 3.0}
{'eval_loss': 0.6031172871589661, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.5921, 'eval_samples_per_second': 28.521, 'eval_steps_per_second': 3.575, 'epoch': 4.0}
{'loss': 0.7747, 'learning_rate': 7.5e-06, 'epoch': 4.17}
{'eval_loss': 0.5570763349533081, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6646, 'eval_samples_per_second': 28.482, 'eval_steps_per_second': 3.57, 'epoch': 5.0}
{'loss': 0.5463, 'learning_rate': 1e-05, 'epoch': 5.56}
{'eval_loss': 0.555627703666687, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6044, 'eval_samples_per_second': 28.515, 'eval_steps_per_second': 3.574, 'epoch': 6.0}
{'loss': 0.413, 'learning_rate': 1.25e-05, 'epoch': 6.95}
{'eval_loss': 0.5348935723304749, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6194, 'eval_samples_per_second': 28.507, 'eval_steps_per_second': 3.573, 'epoch': 7.0}
{'eval_loss': 0.5552850961685181, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.589, 'eval_samples_per_second': 28.523, 'eval_steps_per_second': 3.575, 'epoch': 8.0}
{'loss': 0.2987, 'learning_rate': 1.5e-05, 'epoch': 8.34}
{'eval_loss': 0.5585810542106628, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.7053, 'eval_samples_per_second': 28.46, 'eval_steps_per_second': 3.567, 'epoch': 9.0}
{'loss': 0.2323, 'learning_rate': 1.75e-05, 'epoch': 9.74}
{'eval_loss': 0.5304022431373596, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6329, 'eval_samples_per_second': 28.499, 'eval_steps_per_second': 3.572, 'epoch': 9.99}
{'train_runtime': 5212.222, 'train_samples_per_second': 11.03, 'train_steps_per_second': 0.689, 'train_loss': 1.3503720705887734, 'epoch': 9.99}
Total training time: 5212.24 seconds
######################################################################
layer-wise fine-tuning top 4
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 64.91it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.2', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 3.8200502395629883, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6365, 'eval_samples_per_second': 28.497, 'eval_steps_per_second': 3.572, 'epoch': 1.0}
{'loss': 5.2868, 'learning_rate': 2.5e-06, 'epoch': 1.39}
{'eval_loss': 1.7503536939620972, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.5068, 'eval_samples_per_second': 28.568, 'eval_steps_per_second': 3.58, 'epoch': 2.0}
{'loss': 2.0032, 'learning_rate': 5e-06, 'epoch': 2.78}
{'eval_loss': 0.626040518283844, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.5351, 'eval_samples_per_second': 28.552, 'eval_steps_per_second': 3.579, 'epoch': 3.0}
{'eval_loss': 0.5762860774993896, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.588, 'eval_samples_per_second': 28.524, 'eval_steps_per_second': 3.575, 'epoch': 4.0}
{'loss': 0.6916, 'learning_rate': 7.5e-06, 'epoch': 4.17}
{'eval_loss': 0.5219480991363525, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.5825, 'eval_samples_per_second': 28.527, 'eval_steps_per_second': 3.575, 'epoch': 5.0}
{'loss': 0.4932, 'learning_rate': 1e-05, 'epoch': 5.56}
{'eval_loss': 0.518719494342804, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.5529, 'eval_samples_per_second': 28.543, 'eval_steps_per_second': 3.577, 'epoch': 6.0}
{'loss': 0.369, 'learning_rate': 1.25e-05, 'epoch': 6.95}
{'eval_loss': 0.5169006586074829, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6054, 'eval_samples_per_second': 28.514, 'eval_steps_per_second': 3.574, 'epoch': 7.0}
{'eval_loss': 0.5454881191253662, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.5692, 'eval_samples_per_second': 28.534, 'eval_steps_per_second': 3.576, 'epoch': 8.0}
{'loss': 0.275, 'learning_rate': 1.5e-05, 'epoch': 8.34}
{'eval_loss': 0.5121510624885559, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.6132, 'eval_samples_per_second': 28.51, 'eval_steps_per_second': 3.573, 'epoch': 9.0}
{'loss': 0.214, 'learning_rate': 1.75e-05, 'epoch': 9.74}
{'eval_loss': 0.516548752784729, 'eval_accuracy': 0.22933333333333333, 'eval_runtime': 52.5705, 'eval_samples_per_second': 28.533, 'eval_steps_per_second': 3.576, 'epoch': 9.99}
{'train_runtime': 5333.0922, 'train_samples_per_second': 10.78, 'train_steps_per_second': 0.673, 'train_loss': 1.304433749313142, 'epoch': 9.99}
Total training time: 5333.11 seconds
######################################################################
layer-wise fine-tuning top 5
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 46.83it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
Traceback (most recent call last):
  File "code/fine_tuner.py", line 146, in <module>
    main(args)
  File "code/fine_tuner.py", line 119, in main
    trainer.train()
  File "/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
  File "/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 1929, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 2699, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 2731, in compute_loss
    outputs = model(**inputs)
  File "/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1600, in forward
    loss = loss_fct(logits, labels)
  File "/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py", line 720, in forward
    return F.binary_cross_entropy_with_logits(input, target,
  File "/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 3163, in binary_cross_entropy_with_logits
    raise ValueError("Target size ({}) must be the same as input size ({})".format(target.size(), input.size()))
ValueError: Target size (torch.Size([8])) must be the same as input size (torch.Size([8, 140]))
######################################################################
finished
