######################################################################
layer-wise fine-tuning freezing bottom 5 WIC
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 79.62it/s]
/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.1', 'bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.4')
{'eval_loss': 0.7526454925537109, 'eval_accuracy': 0.5, 'eval_runtime': 20.4797, 'eval_samples_per_second': 31.153, 'eval_steps_per_second': 1.953, 'epoch': 1.0}
{'eval_loss': 0.6830364465713501, 'eval_accuracy': 0.5705329153605015, 'eval_runtime': 20.4304, 'eval_samples_per_second': 31.228, 'eval_steps_per_second': 1.958, 'epoch': 2.0}
{'loss': 0.7351, 'learning_rate': 2.5e-06, 'epoch': 2.94}
{'eval_loss': 0.6682111024856567, 'eval_accuracy': 0.6144200626959248, 'eval_runtime': 20.3207, 'eval_samples_per_second': 31.397, 'eval_steps_per_second': 1.968, 'epoch': 3.0}
{'eval_loss': 0.6690422892570496, 'eval_accuracy': 0.5611285266457681, 'eval_runtime': 20.3505, 'eval_samples_per_second': 31.351, 'eval_steps_per_second': 1.966, 'epoch': 4.0}
{'eval_loss': 0.696079671382904, 'eval_accuracy': 0.5736677115987461, 'eval_runtime': 20.3326, 'eval_samples_per_second': 31.378, 'eval_steps_per_second': 1.967, 'epoch': 5.0}
{'loss': 0.6174, 'learning_rate': 5e-06, 'epoch': 5.88}
{'eval_loss': 0.7094168066978455, 'eval_accuracy': 0.6065830721003135, 'eval_runtime': 20.3712, 'eval_samples_per_second': 31.319, 'eval_steps_per_second': 1.964, 'epoch': 6.0}
{'eval_loss': 0.6735684871673584, 'eval_accuracy': 0.6442006269592476, 'eval_runtime': 20.3397, 'eval_samples_per_second': 31.367, 'eval_steps_per_second': 1.967, 'epoch': 7.0}
{'eval_loss': 0.7038643956184387, 'eval_accuracy': 0.658307210031348, 'eval_runtime': 20.5209, 'eval_samples_per_second': 31.09, 'eval_steps_per_second': 1.949, 'epoch': 8.0}
{'loss': 0.4648, 'learning_rate': 7.5e-06, 'epoch': 8.82}
{'eval_loss': 0.7077645063400269, 'eval_accuracy': 0.6489028213166145, 'eval_runtime': 20.4155, 'eval_samples_per_second': 31.251, 'eval_steps_per_second': 1.959, 'epoch': 9.0}
{'eval_loss': 0.8163087368011475, 'eval_accuracy': 0.6253918495297806, 'eval_runtime': 20.5256, 'eval_samples_per_second': 31.083, 'eval_steps_per_second': 1.949, 'epoch': 10.0}
{'train_runtime': 4522.7028, 'train_samples_per_second': 12.002, 'train_steps_per_second': 0.376, 'train_loss': 0.5750881060431985, 'epoch': 10.0}
Total training time: 4522.73 seconds
######################################################################
layer-wise fine-tuning freezing bottom 5 COLA
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 95.28it/s]
/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.1', 'bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.4')
{'eval_loss': 0.6219155192375183, 'eval_matthews_correlation': 0.0463559874942472, 'eval_runtime': 34.0029, 'eval_samples_per_second': 30.674, 'eval_steps_per_second': 1.941, 'epoch': 1.0}
{'loss': 0.7638, 'learning_rate': 2.5e-06, 'epoch': 1.87}
{'eval_loss': 0.5723843574523926, 'eval_matthews_correlation': 0.14589040785945767, 'eval_runtime': 33.9534, 'eval_samples_per_second': 30.719, 'eval_steps_per_second': 1.944, 'epoch': 2.0}
{'eval_loss': 0.49180737137794495, 'eval_matthews_correlation': 0.4184145751988745, 'eval_runtime': 33.9174, 'eval_samples_per_second': 30.751, 'eval_steps_per_second': 1.946, 'epoch': 3.0}
{'loss': 0.4881, 'learning_rate': 5e-06, 'epoch': 3.74}
{'eval_loss': 0.49246346950531006, 'eval_matthews_correlation': 0.4803905600654982, 'eval_runtime': 33.2743, 'eval_samples_per_second': 31.346, 'eval_steps_per_second': 1.984, 'epoch': 4.0}
{'eval_loss': 0.5090257525444031, 'eval_matthews_correlation': 0.49652146303644756, 'eval_runtime': 33.2936, 'eval_samples_per_second': 31.327, 'eval_steps_per_second': 1.982, 'epoch': 5.0}
{'loss': 0.3774, 'learning_rate': 7.5e-06, 'epoch': 5.61}
{'eval_loss': 0.47393932938575745, 'eval_matthews_correlation': 0.5586744996129839, 'eval_runtime': 33.3048, 'eval_samples_per_second': 31.317, 'eval_steps_per_second': 1.982, 'epoch': 6.0}
{'eval_loss': 0.49896690249443054, 'eval_matthews_correlation': 0.5630262572481878, 'eval_runtime': 33.2868, 'eval_samples_per_second': 31.334, 'eval_steps_per_second': 1.983, 'epoch': 7.0}
{'loss': 0.2942, 'learning_rate': 1e-05, 'epoch': 7.48}
{'eval_loss': 0.5366503596305847, 'eval_matthews_correlation': 0.5598743897878832, 'eval_runtime': 33.2858, 'eval_samples_per_second': 31.335, 'eval_steps_per_second': 1.983, 'epoch': 8.0}
{'eval_loss': 0.5485632419586182, 'eval_matthews_correlation': 0.555285279196476, 'eval_runtime': 33.2778, 'eval_samples_per_second': 31.342, 'eval_steps_per_second': 1.983, 'epoch': 9.0}
{'loss': 0.2169, 'learning_rate': 1.25e-05, 'epoch': 9.35}
{'eval_loss': 0.5457467436790466, 'eval_matthews_correlation': 0.5574138665741355, 'eval_runtime': 33.3124, 'eval_samples_per_second': 31.31, 'eval_steps_per_second': 1.981, 'epoch': 9.98}
{'train_runtime': 7057.9193, 'train_samples_per_second': 12.115, 'train_steps_per_second': 0.378, 'train_loss': 0.4129356655735202, 'epoch': 9.98}
Total training time: 7057.94 seconds
######################################################################
layer-wise fine-tuning freezing bottom 5 WNLI
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 186.63it/s]