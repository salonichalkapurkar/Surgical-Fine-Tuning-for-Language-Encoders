######################################################################
layer-wise fine-tuning freezing bottom 5 CB
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 195.05it/s]
/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.1', 'bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.4')
{'eval_loss': 1.062398910522461, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.6755, 'eval_samples_per_second': 33.422, 'eval_steps_per_second': 2.387, 'epoch': 1.0}
{'eval_loss': 1.0620890855789185, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.6845, 'eval_samples_per_second': 33.243, 'eval_steps_per_second': 2.375, 'epoch': 2.0}
{'eval_loss': 1.0615652799606323, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7001, 'eval_samples_per_second': 32.94, 'eval_steps_per_second': 2.353, 'epoch': 3.0}
{'eval_loss': 1.0608576536178589, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.8335, 'eval_samples_per_second': 30.543, 'eval_steps_per_second': 2.182, 'epoch': 4.0}
{'eval_loss': 1.0599700212478638, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.8285, 'eval_samples_per_second': 30.625, 'eval_steps_per_second': 2.188, 'epoch': 5.0}
{'eval_loss': 1.0589243173599243, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.835, 'eval_samples_per_second': 30.518, 'eval_steps_per_second': 2.18, 'epoch': 6.0}
{'eval_loss': 1.0577551126480103, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.841, 'eval_samples_per_second': 30.419, 'eval_steps_per_second': 2.173, 'epoch': 7.0}
{'eval_loss': 1.0564626455307007, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.8266, 'eval_samples_per_second': 30.657, 'eval_steps_per_second': 2.19, 'epoch': 8.0}
{'eval_loss': 1.0549699068069458, 'eval_accuracy': 0.4107142857142857, 'eval_runtime': 1.8328, 'eval_samples_per_second': 30.555, 'eval_steps_per_second': 2.183, 'epoch': 9.0}
{'eval_loss': 1.053213357925415, 'eval_accuracy': 0.4107142857142857, 'eval_runtime': 1.829, 'eval_samples_per_second': 30.617, 'eval_steps_per_second': 2.187, 'epoch': 10.0}
{'train_runtime': 301.2011, 'train_samples_per_second': 8.3, 'train_steps_per_second': 0.266, 'train_loss': 1.0250070571899415, 'epoch': 10.0}
Total training time: 301.21 seconds
######################################################################
layer-wise fine-tuning freezing bottom 5 COPA
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 194.97it/s]
/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.1', 'bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.4')
Traceback (most recent call last):
  File "code/fine_tuner.py", line 189, in <module>
    main(args)
  File "code/fine_tuner.py", line 146, in main
    trainer.train()
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 1633, in train
    return inner_training_loop(
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 1902, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 2645, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 2677, in compute_loss
    outputs = model(**inputs)
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1672, in forward
    outputs = self.bert(
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1020, in forward
    encoder_outputs = self.encoder(
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 610, in forward
    layer_outputs = layer_module(
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 495, in forward
    self_attention_outputs = self.attention(
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 425, in forward
    self_outputs = self.self(
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 357, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1252, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 11.93 GiB total capacity; 11.32 GiB already allocated; 15.06 MiB free; 11.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
######################################################################
finished
