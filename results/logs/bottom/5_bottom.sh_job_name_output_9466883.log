######################################################################
layer-wise fine-tuning freezing bottom 5 WNLI
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 359.79it/s]
/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.1', 'bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.4')
{'eval_loss': 0.8019618988037109, 'eval_accuracy': 0.5633802816901409, 'eval_runtime': 2.1362, 'eval_samples_per_second': 33.237, 'eval_steps_per_second': 2.341, 'epoch': 1.0}
{'eval_loss': 0.8000050783157349, 'eval_accuracy': 0.5633802816901409, 'eval_runtime': 2.2964, 'eval_samples_per_second': 30.918, 'eval_steps_per_second': 2.177, 'epoch': 2.0}
{'eval_loss': 0.796722412109375, 'eval_accuracy': 0.5633802816901409, 'eval_runtime': 2.3108, 'eval_samples_per_second': 30.726, 'eval_steps_per_second': 2.164, 'epoch': 3.0}
{'eval_loss': 0.7916620373725891, 'eval_accuracy': 0.5633802816901409, 'eval_runtime': 2.317, 'eval_samples_per_second': 30.643, 'eval_steps_per_second': 2.158, 'epoch': 4.0}
{'eval_loss': 0.7846090197563171, 'eval_accuracy': 0.5633802816901409, 'eval_runtime': 2.3156, 'eval_samples_per_second': 30.661, 'eval_steps_per_second': 2.159, 'epoch': 5.0}
{'eval_loss': 0.7746424078941345, 'eval_accuracy': 0.5633802816901409, 'eval_runtime': 2.3233, 'eval_samples_per_second': 30.56, 'eval_steps_per_second': 2.152, 'epoch': 6.0}
{'eval_loss': 0.7554261088371277, 'eval_accuracy': 0.5633802816901409, 'eval_runtime': 2.3188, 'eval_samples_per_second': 30.619, 'eval_steps_per_second': 2.156, 'epoch': 7.0}
{'eval_loss': 0.7246527671813965, 'eval_accuracy': 0.5633802816901409, 'eval_runtime': 2.3172, 'eval_samples_per_second': 30.641, 'eval_steps_per_second': 2.158, 'epoch': 8.0}
{'eval_loss': 0.6987216472625732, 'eval_accuracy': 0.5633802816901409, 'eval_runtime': 2.3107, 'eval_samples_per_second': 30.726, 'eval_steps_per_second': 2.164, 'epoch': 9.0}
{'eval_loss': 0.6900111436843872, 'eval_accuracy': 0.5633802816901409, 'eval_runtime': 2.3201, 'eval_samples_per_second': 30.602, 'eval_steps_per_second': 2.155, 'epoch': 10.0}
{'train_runtime': 605.7412, 'train_samples_per_second': 10.483, 'train_steps_per_second': 0.33, 'train_loss': 0.8361286926269531, 'epoch': 10.0}
Total training time: 605.75 seconds
######################################################################
layer-wise fine-tuning freezing bottom 5 RTE
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 252.24it/s]
/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.1', 'bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.4')
{'eval_loss': 0.8289812803268433, 'eval_accuracy': 0.5270758122743683, 'eval_runtime': 8.9709, 'eval_samples_per_second': 30.878, 'eval_steps_per_second': 2.006, 'epoch': 1.0}
{'eval_loss': 0.7896838188171387, 'eval_accuracy': 0.5270758122743683, 'eval_runtime': 8.987, 'eval_samples_per_second': 30.822, 'eval_steps_per_second': 2.003, 'epoch': 2.0}
{'eval_loss': 0.7078760266304016, 'eval_accuracy': 0.5270758122743683, 'eval_runtime': 8.9785, 'eval_samples_per_second': 30.851, 'eval_steps_per_second': 2.005, 'epoch': 3.0}
{'eval_loss': 0.6912611722946167, 'eval_accuracy': 0.5126353790613718, 'eval_runtime': 8.9778, 'eval_samples_per_second': 30.854, 'eval_steps_per_second': 2.005, 'epoch': 4.0}
{'eval_loss': 0.6861308813095093, 'eval_accuracy': 0.5306859205776173, 'eval_runtime': 8.9732, 'eval_samples_per_second': 30.87, 'eval_steps_per_second': 2.006, 'epoch': 5.0}
{'eval_loss': 0.6755016446113586, 'eval_accuracy': 0.5848375451263538, 'eval_runtime': 8.9671, 'eval_samples_per_second': 30.891, 'eval_steps_per_second': 2.007, 'epoch': 6.0}
{'loss': 0.7575, 'learning_rate': 2.5e-06, 'epoch': 6.41}
{'eval_loss': 0.6650220155715942, 'eval_accuracy': 0.5812274368231047, 'eval_runtime': 8.976, 'eval_samples_per_second': 30.86, 'eval_steps_per_second': 2.005, 'epoch': 7.0}
{'eval_loss': 0.655458927154541, 'eval_accuracy': 0.5956678700361011, 'eval_runtime': 8.9633, 'eval_samples_per_second': 30.904, 'eval_steps_per_second': 2.008, 'epoch': 8.0}
{'eval_loss': 0.6471344828605652, 'eval_accuracy': 0.6028880866425993, 'eval_runtime': 8.965, 'eval_samples_per_second': 30.898, 'eval_steps_per_second': 2.008, 'epoch': 9.0}
{'eval_loss': 0.6460204124450684, 'eval_accuracy': 0.6101083032490975, 'eval_runtime': 8.9573, 'eval_samples_per_second': 30.925, 'eval_steps_per_second': 2.01, 'epoch': 10.0}
{'train_runtime': 2153.6915, 'train_samples_per_second': 11.562, 'train_steps_per_second': 0.362, 'train_loss': 0.7058356651893029, 'epoch': 10.0}
Total training time: 2153.70 seconds
######################################################################
finished
