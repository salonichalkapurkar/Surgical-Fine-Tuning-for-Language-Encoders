######################################################################
full model fine-tuning
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 25.86it/s]100%|██████████| 3/3 [00:00<00:00, 25.78it/s]
Map:   0%|          | 0/2490 [00:00<?, ? examples/s]Map:  40%|████      | 1000/2490 [00:00<00:00, 2310.16 examples/s]Map:  80%|████████  | 2000/2490 [00:00<00:00, 2853.85 examples/s]Map: 100%|██████████| 2490/2490 [00:00<00:00, 3050.35 examples/s]                                                                 Map:   0%|          | 0/277 [00:00<?, ? examples/s]Map: 100%|██████████| 277/277 [00:00<00:00, 2744.95 examples/s]                                                               /home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'eval_loss': 0.6988105773925781, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 9.1564, 'eval_samples_per_second': 30.252, 'eval_steps_per_second': 1.966, 'epoch': 1.0}
{'eval_loss': 0.6976685523986816, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 9.1343, 'eval_samples_per_second': 30.325, 'eval_steps_per_second': 1.971, 'epoch': 2.0}
{'eval_loss': 0.6962223052978516, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 9.1508, 'eval_samples_per_second': 30.271, 'eval_steps_per_second': 1.967, 'epoch': 3.0}
{'eval_loss': 0.6949279308319092, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 9.1244, 'eval_samples_per_second': 30.358, 'eval_steps_per_second': 1.973, 'epoch': 4.0}
{'eval_loss': 0.6936250329017639, 'eval_accuracy': 0.48375451263537905, 'eval_runtime': 9.1128, 'eval_samples_per_second': 30.397, 'eval_steps_per_second': 1.975, 'epoch': 5.0}
{'eval_loss': 0.6907681226730347, 'eval_accuracy': 0.5451263537906137, 'eval_runtime': 9.1116, 'eval_samples_per_second': 30.401, 'eval_steps_per_second': 1.975, 'epoch': 6.0}
{'loss': 0.6941, 'learning_rate': 2.5e-06, 'epoch': 6.41}
{'eval_loss': 0.6726986169815063, 'eval_accuracy': 0.5631768953068592, 'eval_runtime': 9.1135, 'eval_samples_per_second': 30.394, 'eval_steps_per_second': 1.975, 'epoch': 7.0}
{'eval_loss': 0.626800000667572, 'eval_accuracy': 0.6606498194945848, 'eval_runtime': 9.123, 'eval_samples_per_second': 30.363, 'eval_steps_per_second': 1.973, 'epoch': 8.0}
{'eval_loss': 0.573529839515686, 'eval_accuracy': 0.6823104693140795, 'eval_runtime': 9.1076, 'eval_samples_per_second': 30.414, 'eval_steps_per_second': 1.976, 'epoch': 9.0}
{'eval_loss': 0.5739506483078003, 'eval_accuracy': 0.7003610108303249, 'eval_runtime': 9.1993, 'eval_samples_per_second': 30.111, 'eval_steps_per_second': 1.957, 'epoch': 10.0}
{'train_runtime': 2523.089, 'train_samples_per_second': 9.869, 'train_steps_per_second': 0.309, 'train_loss': 0.6560771257449419, 'epoch': 10.0}
Total training time: 2523.12 seconds
######################################################################
layer-wise fine-tuning top 1
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 55.01it/s]
Map:   0%|          | 0/277 [00:00<?, ? examples/s]Map: 100%|██████████| 277/277 [00:00<00:00, 2547.06 examples/s]                                                               /home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('roberta.encoder.layer.0', 'roberta.encoder.layer.1', 'roberta.encoder.layer.2', 'roberta.encoder.layer.3', 'roberta.encoder.layer.4', 'roberta.encoder.layer.5', 'roberta.encoder.layer.6', 'roberta.encoder.layer.7', 'roberta.encoder.layer.8', 'roberta.encoder.layer.9', 'roberta.encoder.layer.10')
{'eval_loss': 0.6993681192398071, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9677, 'eval_samples_per_second': 30.889, 'eval_steps_per_second': 2.007, 'epoch': 1.0}
{'eval_loss': 0.6989327669143677, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9636, 'eval_samples_per_second': 30.903, 'eval_steps_per_second': 2.008, 'epoch': 2.0}
{'eval_loss': 0.6980277895927429, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.984, 'eval_samples_per_second': 30.833, 'eval_steps_per_second': 2.004, 'epoch': 3.0}
{'eval_loss': 0.6971980929374695, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9755, 'eval_samples_per_second': 30.862, 'eval_steps_per_second': 2.005, 'epoch': 4.0}
{'eval_loss': 0.6964322924613953, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9621, 'eval_samples_per_second': 30.908, 'eval_steps_per_second': 2.008, 'epoch': 5.0}
{'eval_loss': 0.6958152651786804, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9771, 'eval_samples_per_second': 30.856, 'eval_steps_per_second': 2.005, 'epoch': 6.0}
{'loss': 0.6956, 'learning_rate': 2.5e-06, 'epoch': 6.41}
{'eval_loss': 0.6951172351837158, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9642, 'eval_samples_per_second': 30.901, 'eval_steps_per_second': 2.008, 'epoch': 7.0}
{'eval_loss': 0.6946365237236023, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9868, 'eval_samples_per_second': 30.823, 'eval_steps_per_second': 2.003, 'epoch': 8.0}
{'eval_loss': 0.6940497159957886, 'eval_accuracy': 0.4657039711191336, 'eval_runtime': 8.9738, 'eval_samples_per_second': 30.868, 'eval_steps_per_second': 2.006, 'epoch': 9.0}
{'eval_loss': 0.6937276124954224, 'eval_accuracy': 0.48736462093862815, 'eval_runtime': 8.9721, 'eval_samples_per_second': 30.873, 'eval_steps_per_second': 2.006, 'epoch': 10.0}
{'train_runtime': 1928.6733, 'train_samples_per_second': 12.91, 'train_steps_per_second': 0.404, 'train_loss': 0.6949746156350161, 'epoch': 10.0}
Total training time: 1928.69 seconds
######################################################################
layer-wise fine-tuning top 2
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 59.04it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('roberta.encoder.layer.0', 'roberta.encoder.layer.1', 'roberta.encoder.layer.2', 'roberta.encoder.layer.3', 'roberta.encoder.layer.4', 'roberta.encoder.layer.5', 'roberta.encoder.layer.6', 'roberta.encoder.layer.7', 'roberta.encoder.layer.8', 'roberta.encoder.layer.9')
{'eval_loss': 0.6993681192398071, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9983, 'eval_samples_per_second': 30.784, 'eval_steps_per_second': 2.0, 'epoch': 1.0}
{'eval_loss': 0.6989327669143677, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9877, 'eval_samples_per_second': 30.82, 'eval_steps_per_second': 2.003, 'epoch': 2.0}
{'eval_loss': 0.6980277895927429, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9803, 'eval_samples_per_second': 30.845, 'eval_steps_per_second': 2.004, 'epoch': 3.0}
{'eval_loss': 0.6971980929374695, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9618, 'eval_samples_per_second': 30.909, 'eval_steps_per_second': 2.009, 'epoch': 4.0}
{'eval_loss': 0.6964322924613953, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9722, 'eval_samples_per_second': 30.873, 'eval_steps_per_second': 2.006, 'epoch': 5.0}
{'eval_loss': 0.6958152651786804, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9684, 'eval_samples_per_second': 30.886, 'eval_steps_per_second': 2.007, 'epoch': 6.0}
{'loss': 0.6956, 'learning_rate': 2.5e-06, 'epoch': 6.41}
{'eval_loss': 0.6951172351837158, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9765, 'eval_samples_per_second': 30.858, 'eval_steps_per_second': 2.005, 'epoch': 7.0}
{'eval_loss': 0.6946365237236023, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9846, 'eval_samples_per_second': 30.831, 'eval_steps_per_second': 2.003, 'epoch': 8.0}
{'eval_loss': 0.6940497159957886, 'eval_accuracy': 0.4657039711191336, 'eval_runtime': 8.9751, 'eval_samples_per_second': 30.863, 'eval_steps_per_second': 2.006, 'epoch': 9.0}
{'eval_loss': 0.6937276124954224, 'eval_accuracy': 0.48736462093862815, 'eval_runtime': 8.9812, 'eval_samples_per_second': 30.842, 'eval_steps_per_second': 2.004, 'epoch': 10.0}
{'train_runtime': 1934.1791, 'train_samples_per_second': 12.874, 'train_steps_per_second': 0.403, 'train_loss': 0.6949746156350161, 'epoch': 10.0}
Total training time: 1934.20 seconds
######################################################################
layer-wise fine-tuning top 3
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 58.21it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('roberta.encoder.layer.0', 'roberta.encoder.layer.1', 'roberta.encoder.layer.2', 'roberta.encoder.layer.3', 'roberta.encoder.layer.4', 'roberta.encoder.layer.5', 'roberta.encoder.layer.6', 'roberta.encoder.layer.7', 'roberta.encoder.layer.8')
{'eval_loss': 0.6993244290351868, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 9.0054, 'eval_samples_per_second': 30.759, 'eval_steps_per_second': 1.999, 'epoch': 1.0}
{'eval_loss': 0.6988276243209839, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9867, 'eval_samples_per_second': 30.823, 'eval_steps_per_second': 2.003, 'epoch': 2.0}
{'eval_loss': 0.6978206634521484, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9723, 'eval_samples_per_second': 30.873, 'eval_steps_per_second': 2.006, 'epoch': 3.0}
{'eval_loss': 0.6969161629676819, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9813, 'eval_samples_per_second': 30.842, 'eval_steps_per_second': 2.004, 'epoch': 4.0}
{'eval_loss': 0.6961426138877869, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9883, 'eval_samples_per_second': 30.818, 'eval_steps_per_second': 2.003, 'epoch': 5.0}
{'eval_loss': 0.6954712867736816, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9919, 'eval_samples_per_second': 30.805, 'eval_steps_per_second': 2.002, 'epoch': 6.0}
{'loss': 0.6954, 'learning_rate': 2.5e-06, 'epoch': 6.41}
{'eval_loss': 0.6947086453437805, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9829, 'eval_samples_per_second': 30.836, 'eval_steps_per_second': 2.004, 'epoch': 7.0}
{'eval_loss': 0.6941759586334229, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 9.0082, 'eval_samples_per_second': 30.75, 'eval_steps_per_second': 1.998, 'epoch': 8.0}
{'eval_loss': 0.69353187084198, 'eval_accuracy': 0.48375451263537905, 'eval_runtime': 9.0012, 'eval_samples_per_second': 30.774, 'eval_steps_per_second': 2.0, 'epoch': 9.0}
{'eval_loss': 0.6931766271591187, 'eval_accuracy': 0.48014440433212996, 'eval_runtime': 9.0092, 'eval_samples_per_second': 30.746, 'eval_steps_per_second': 1.998, 'epoch': 10.0}
{'train_runtime': 1978.9605, 'train_samples_per_second': 12.582, 'train_steps_per_second': 0.394, 'train_loss': 0.6946719242976263, 'epoch': 10.0}
Total training time: 1978.98 seconds
######################################################################
layer-wise fine-tuning top 4
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 64.48it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('roberta.encoder.layer.0', 'roberta.encoder.layer.1', 'roberta.encoder.layer.2', 'roberta.encoder.layer.3', 'roberta.encoder.layer.4', 'roberta.encoder.layer.5', 'roberta.encoder.layer.6', 'roberta.encoder.layer.7')
{'eval_loss': 0.6992917060852051, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 9.0311, 'eval_samples_per_second': 30.672, 'eval_steps_per_second': 1.993, 'epoch': 1.0}
{'eval_loss': 0.6987497210502625, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 9.0299, 'eval_samples_per_second': 30.676, 'eval_steps_per_second': 1.993, 'epoch': 2.0}
{'eval_loss': 0.6976741552352905, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 9.0095, 'eval_samples_per_second': 30.745, 'eval_steps_per_second': 1.998, 'epoch': 3.0}
{'eval_loss': 0.6967049837112427, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 9.0173, 'eval_samples_per_second': 30.719, 'eval_steps_per_second': 1.996, 'epoch': 4.0}
{'eval_loss': 0.6959166526794434, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 9.0161, 'eval_samples_per_second': 30.723, 'eval_steps_per_second': 1.996, 'epoch': 5.0}
{'eval_loss': 0.6951919794082642, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9978, 'eval_samples_per_second': 30.785, 'eval_steps_per_second': 2.0, 'epoch': 6.0}
{'loss': 0.6953, 'learning_rate': 2.5e-06, 'epoch': 6.41}
{'eval_loss': 0.6943628191947937, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.994, 'eval_samples_per_second': 30.798, 'eval_steps_per_second': 2.001, 'epoch': 7.0}
{'eval_loss': 0.693760097026825, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 9.0088, 'eval_samples_per_second': 30.748, 'eval_steps_per_second': 1.998, 'epoch': 8.0}
{'eval_loss': 0.6930133104324341, 'eval_accuracy': 0.49097472924187724, 'eval_runtime': 9.006, 'eval_samples_per_second': 30.757, 'eval_steps_per_second': 1.999, 'epoch': 9.0}
{'eval_loss': 0.6924943923950195, 'eval_accuracy': 0.4981949458483754, 'eval_runtime': 9.0041, 'eval_samples_per_second': 30.764, 'eval_steps_per_second': 1.999, 'epoch': 10.0}
{'train_runtime': 2031.6923, 'train_samples_per_second': 12.256, 'train_steps_per_second': 0.384, 'train_loss': 0.6944001222268129, 'epoch': 10.0}
Total training time: 2031.71 seconds
######################################################################
layer-wise fine-tuning top 5
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 68.93it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('roberta.encoder.layer.0', 'roberta.encoder.layer.1', 'roberta.encoder.layer.2', 'roberta.encoder.layer.4', 'roberta.encoder.layer.5', 'roberta.encoder.layer.6', 'roberta.encoder.layer.7')
{'eval_loss': 0.6992810964584351, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 9.0568, 'eval_samples_per_second': 30.585, 'eval_steps_per_second': 1.987, 'epoch': 1.0}
{'eval_loss': 0.6987224817276001, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 9.0239, 'eval_samples_per_second': 30.696, 'eval_steps_per_second': 1.995, 'epoch': 2.0}
{'eval_loss': 0.6976183652877808, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 9.02, 'eval_samples_per_second': 30.71, 'eval_steps_per_second': 1.996, 'epoch': 3.0}
{'eval_loss': 0.6966411471366882, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 9.001, 'eval_samples_per_second': 30.774, 'eval_steps_per_second': 2.0, 'epoch': 4.0}
{'eval_loss': 0.6958445310592651, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 9.0224, 'eval_samples_per_second': 30.701, 'eval_steps_per_second': 1.995, 'epoch': 5.0}
{'eval_loss': 0.6951059103012085, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 9.0221, 'eval_samples_per_second': 30.702, 'eval_steps_per_second': 1.995, 'epoch': 6.0}
{'loss': 0.6953, 'learning_rate': 2.5e-06, 'epoch': 6.41}
{'eval_loss': 0.6942113041877747, 'eval_accuracy': 0.4729241877256318, 'eval_runtime': 8.9976, 'eval_samples_per_second': 30.786, 'eval_steps_per_second': 2.001, 'epoch': 7.0}
{'eval_loss': 0.6935235261917114, 'eval_accuracy': 0.49097472924187724, 'eval_runtime': 9.034, 'eval_samples_per_second': 30.662, 'eval_steps_per_second': 1.992, 'epoch': 8.0}
{'eval_loss': 0.6925918459892273, 'eval_accuracy': 0.516245487364621, 'eval_runtime': 9.0137, 'eval_samples_per_second': 30.731, 'eval_steps_per_second': 1.997, 'epoch': 9.0}
{'eval_loss': 0.6916103959083557, 'eval_accuracy': 0.5487364620938628, 'eval_runtime': 9.0203, 'eval_samples_per_second': 30.709, 'eval_steps_per_second': 1.996, 'epoch': 10.0}
{'train_runtime': 2074.6585, 'train_samples_per_second': 12.002, 'train_steps_per_second': 0.376, 'train_loss': 0.6942146496895032, 'epoch': 10.0}
Total training time: 2074.68 seconds
######################################################################
finished
