######################################################################
layer-wise fine-tuning bottom 1 - rte
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 241.36it/s]
/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.1', 'bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.4', 'bert.encoder.layer.5', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10')
{'eval_loss': 0.8322306275367737, 'eval_accuracy': 0.5270758122743683, 'eval_runtime': 8.3935, 'eval_samples_per_second': 33.002, 'eval_steps_per_second': 2.145, 'epoch': 1.0}
{'eval_loss': 0.8101941347122192, 'eval_accuracy': 0.5270758122743683, 'eval_runtime': 8.3914, 'eval_samples_per_second': 33.01, 'eval_steps_per_second': 2.145, 'epoch': 2.0}
{'eval_loss': 0.7771229147911072, 'eval_accuracy': 0.5270758122743683, 'eval_runtime': 8.3935, 'eval_samples_per_second': 33.002, 'eval_steps_per_second': 2.145, 'epoch': 3.0}
{'eval_loss': 0.7393730282783508, 'eval_accuracy': 0.5270758122743683, 'eval_runtime': 8.3976, 'eval_samples_per_second': 32.985, 'eval_steps_per_second': 2.143, 'epoch': 4.0}
{'eval_loss': 0.7073293924331665, 'eval_accuracy': 0.5270758122743683, 'eval_runtime': 8.4006, 'eval_samples_per_second': 32.974, 'eval_steps_per_second': 2.143, 'epoch': 5.0}
{'eval_loss': 0.6912009716033936, 'eval_accuracy': 0.5342960288808665, 'eval_runtime': 8.3866, 'eval_samples_per_second': 33.029, 'eval_steps_per_second': 2.146, 'epoch': 6.0}
{'loss': 0.7967, 'learning_rate': 2.5e-06, 'epoch': 6.41}
{'eval_loss': 0.6886411905288696, 'eval_accuracy': 0.5415162454873647, 'eval_runtime': 8.4072, 'eval_samples_per_second': 32.948, 'eval_steps_per_second': 2.141, 'epoch': 7.0}
{'eval_loss': 0.6888529658317566, 'eval_accuracy': 0.5703971119133574, 'eval_runtime': 8.4024, 'eval_samples_per_second': 32.967, 'eval_steps_per_second': 2.142, 'epoch': 8.0}
{'eval_loss': 0.689497709274292, 'eval_accuracy': 0.5776173285198556, 'eval_runtime': 8.3944, 'eval_samples_per_second': 32.998, 'eval_steps_per_second': 2.144, 'epoch': 9.0}
{'eval_loss': 0.6901286244392395, 'eval_accuracy': 0.5595667870036101, 'eval_runtime': 8.4041, 'eval_samples_per_second': 32.96, 'eval_steps_per_second': 2.142, 'epoch': 10.0}
{'train_runtime': 1849.3595, 'train_samples_per_second': 13.464, 'train_steps_per_second': 0.422, 'train_loss': 0.7582198314177684, 'epoch': 10.0}
Total training time: 1849.38 seconds
######################################################################
layer-wise fine-tuning bottom 1 - cola
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
Downloading builder script: 0.00B [00:00, ?B/s]Downloading builder script: 6.60kB [00:00, 4.76MB/s]
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 95.71it/s]
/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.1', 'bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.4', 'bert.encoder.layer.5', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10')
{'eval_loss': 0.8776024580001831, 'eval_matthews_correlation': 0.0, 'eval_runtime': 31.5832, 'eval_samples_per_second': 33.024, 'eval_steps_per_second': 2.09, 'epoch': 1.0}
{'loss': 0.8866, 'learning_rate': 2.5e-06, 'epoch': 1.87}
{'eval_loss': 0.6291619539260864, 'eval_matthews_correlation': 0.0, 'eval_runtime': 31.5895, 'eval_samples_per_second': 33.017, 'eval_steps_per_second': 2.089, 'epoch': 2.0}
{'eval_loss': 0.6128185987472534, 'eval_matthews_correlation': 0.0, 'eval_runtime': 31.6187, 'eval_samples_per_second': 32.987, 'eval_steps_per_second': 2.087, 'epoch': 3.0}
{'loss': 0.6093, 'learning_rate': 5e-06, 'epoch': 3.74}
{'eval_loss': 0.6090160608291626, 'eval_matthews_correlation': 0.0, 'eval_runtime': 31.5851, 'eval_samples_per_second': 33.022, 'eval_steps_per_second': 2.09, 'epoch': 4.0}
{'eval_loss': 0.6056569814682007, 'eval_matthews_correlation': 0.0, 'eval_runtime': 31.6184, 'eval_samples_per_second': 32.987, 'eval_steps_per_second': 2.087, 'epoch': 5.0}
{'loss': 0.6002, 'learning_rate': 7.5e-06, 'epoch': 5.61}
{'eval_loss': 0.5992991328239441, 'eval_matthews_correlation': 0.0, 'eval_runtime': 31.6244, 'eval_samples_per_second': 32.981, 'eval_steps_per_second': 2.087, 'epoch': 6.0}
{'eval_loss': 0.5953161716461182, 'eval_matthews_correlation': 0.0, 'eval_runtime': 31.6333, 'eval_samples_per_second': 32.972, 'eval_steps_per_second': 2.086, 'epoch': 7.0}
{'loss': 0.586, 'learning_rate': 1e-05, 'epoch': 7.48}
{'eval_loss': 0.5831382870674133, 'eval_matthews_correlation': 0.0, 'eval_runtime': 31.6588, 'eval_samples_per_second': 32.945, 'eval_steps_per_second': 2.085, 'epoch': 8.0}
{'eval_loss': 0.5725709795951843, 'eval_matthews_correlation': 0.1077683109552469, 'eval_runtime': 31.6372, 'eval_samples_per_second': 32.968, 'eval_steps_per_second': 2.086, 'epoch': 9.0}
{'loss': 0.5656, 'learning_rate': 1.25e-05, 'epoch': 9.35}
{'eval_loss': 0.5553687810897827, 'eval_matthews_correlation': 0.20784212350901957, 'eval_runtime': 31.6651, 'eval_samples_per_second': 32.938, 'eval_steps_per_second': 2.084, 'epoch': 9.98}
{'train_runtime': 6209.3237, 'train_samples_per_second': 13.771, 'train_steps_per_second': 0.43, 'train_loss': 0.6429689700237374, 'epoch': 9.98}
Total training time: 6209.34 seconds
######################################################################
layer-wise fine-tuning bottom 1 - wnli
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 153.83it/s]
/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.1', 'bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.4', 'bert.encoder.layer.5', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10')
{'eval_loss': 0.8021393418312073, 'eval_accuracy': 0.5633802816901409, 'eval_runtime': 2.18, 'eval_samples_per_second': 32.568, 'eval_steps_per_second': 2.294, 'epoch': 1.0}
{'eval_loss': 0.8007723093032837, 'eval_accuracy': 0.5633802816901409, 'eval_runtime': 2.1952, 'eval_samples_per_second': 32.343, 'eval_steps_per_second': 2.278, 'epoch': 2.0}
{'eval_loss': 0.7985106110572815, 'eval_accuracy': 0.5633802816901409, 'eval_runtime': 2.176, 'eval_samples_per_second': 32.628, 'eval_steps_per_second': 2.298, 'epoch': 3.0}
{'eval_loss': 0.7953583598136902, 'eval_accuracy': 0.5633802816901409, 'eval_runtime': 2.1798, 'eval_samples_per_second': 32.572, 'eval_steps_per_second': 2.294, 'epoch': 4.0}
{'eval_loss': 0.7913514971733093, 'eval_accuracy': 0.5633802816901409, 'eval_runtime': 2.1841, 'eval_samples_per_second': 32.508, 'eval_steps_per_second': 2.289, 'epoch': 5.0}
{'eval_loss': 0.7865816354751587, 'eval_accuracy': 0.5633802816901409, 'eval_runtime': 2.1824, 'eval_samples_per_second': 32.533, 'eval_steps_per_second': 2.291, 'epoch': 6.0}
{'eval_loss': 0.7810227870941162, 'eval_accuracy': 0.5633802816901409, 'eval_runtime': 2.1829, 'eval_samples_per_second': 32.525, 'eval_steps_per_second': 2.291, 'epoch': 7.0}
{'eval_loss': 0.7747412919998169, 'eval_accuracy': 0.5633802816901409, 'eval_runtime': 2.173, 'eval_samples_per_second': 32.673, 'eval_steps_per_second': 2.301, 'epoch': 8.0}
{'eval_loss': 0.7678599953651428, 'eval_accuracy': 0.5633802816901409, 'eval_runtime': 2.1817, 'eval_samples_per_second': 32.543, 'eval_steps_per_second': 2.292, 'epoch': 9.0}
{'eval_loss': 0.7604332566261292, 'eval_accuracy': 0.5633802816901409, 'eval_runtime': 2.1841, 'eval_samples_per_second': 32.507, 'eval_steps_per_second': 2.289, 'epoch': 10.0}
{'train_runtime': 521.4763, 'train_samples_per_second': 12.177, 'train_steps_per_second': 0.384, 'train_loss': 0.861138916015625, 'epoch': 10.0}
Total training time: 521.50 seconds
######################################################################
finished
