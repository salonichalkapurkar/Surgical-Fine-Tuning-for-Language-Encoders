######################################################################
full model fine-tuning
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
Downloading builder script:   0%|          | 0.00/6.60k [00:00<?, ?B/s]Downloading builder script: 100%|██████████| 6.60k/6.60k [00:00<00:00, 8.57MB/s]
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 37.51it/s]
Map:   0%|          | 0/8551 [00:00<?, ? examples/s]Map:  12%|█▏        | 1000/8551 [00:00<00:02, 2818.02 examples/s]Map:  23%|██▎       | 2000/8551 [00:00<00:01, 3668.44 examples/s]Map:  35%|███▌      | 3000/8551 [00:00<00:01, 4502.08 examples/s]Map:  47%|████▋     | 4000/8551 [00:00<00:00, 5132.26 examples/s]Map:  58%|█████▊    | 5000/8551 [00:01<00:00, 5510.99 examples/s]Map:  70%|███████   | 6000/8551 [00:01<00:00, 4466.86 examples/s]Map:  82%|████████▏ | 7000/8551 [00:01<00:00, 4951.09 examples/s]Map:  94%|█████████▎| 8000/8551 [00:01<00:00, 5327.45 examples/s]                                                                 Map:   0%|          | 0/1043 [00:00<?, ? examples/s]Map:  96%|█████████▌| 1000/1043 [00:00<00:00, 6421.57 examples/s]                                                                 /home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'eval_loss': 0.6107814908027649, 'eval_matthews_correlation': 0.0, 'eval_runtime': 34.3326, 'eval_samples_per_second': 30.379, 'eval_steps_per_second': 1.922, 'epoch': 1.0}
{'loss': 0.6177, 'learning_rate': 2.5e-06, 'epoch': 1.87}
{'eval_loss': 0.5276503562927246, 'eval_matthews_correlation': 0.0, 'eval_runtime': 34.2748, 'eval_samples_per_second': 30.431, 'eval_steps_per_second': 1.926, 'epoch': 2.0}
{'eval_loss': 0.5077077746391296, 'eval_matthews_correlation': 0.46133824119384026, 'eval_runtime': 34.2791, 'eval_samples_per_second': 30.427, 'eval_steps_per_second': 1.925, 'epoch': 3.0}
{'loss': 0.4528, 'learning_rate': 5e-06, 'epoch': 3.74}
{'eval_loss': 0.49722760915756226, 'eval_matthews_correlation': 0.5101050627100728, 'eval_runtime': 34.287, 'eval_samples_per_second': 30.42, 'eval_steps_per_second': 1.925, 'epoch': 4.0}
{'eval_loss': 0.5052921772003174, 'eval_matthews_correlation': 0.5390322970786539, 'eval_runtime': 34.2555, 'eval_samples_per_second': 30.448, 'eval_steps_per_second': 1.927, 'epoch': 5.0}
{'loss': 0.3177, 'learning_rate': 7.5e-06, 'epoch': 5.61}
{'eval_loss': 0.4301542341709137, 'eval_matthews_correlation': 0.5820159563417776, 'eval_runtime': 34.313, 'eval_samples_per_second': 30.397, 'eval_steps_per_second': 1.923, 'epoch': 6.0}
{'eval_loss': 0.5020393133163452, 'eval_matthews_correlation': 0.5649812173768567, 'eval_runtime': 34.2861, 'eval_samples_per_second': 30.421, 'eval_steps_per_second': 1.925, 'epoch': 7.0}
{'loss': 0.2282, 'learning_rate': 1e-05, 'epoch': 7.48}
{'eval_loss': 0.4952028691768646, 'eval_matthews_correlation': 0.6019614846840845, 'eval_runtime': 34.2881, 'eval_samples_per_second': 30.419, 'eval_steps_per_second': 1.925, 'epoch': 8.0}
{'eval_loss': 0.5572414398193359, 'eval_matthews_correlation': 0.6149885976857739, 'eval_runtime': 34.2838, 'eval_samples_per_second': 30.423, 'eval_steps_per_second': 1.925, 'epoch': 9.0}
{'loss': 0.1708, 'learning_rate': 1.25e-05, 'epoch': 9.35}
{'eval_loss': 0.5732566714286804, 'eval_matthews_correlation': 0.5576106804001113, 'eval_runtime': 34.2782, 'eval_samples_per_second': 30.427, 'eval_steps_per_second': 1.925, 'epoch': 9.98}
{'train_runtime': 8273.5727, 'train_samples_per_second': 10.335, 'train_steps_per_second': 0.323, 'train_loss': 0.3446673125363468, 'epoch': 9.98}
Total training time: 8273.61 seconds
######################################################################
layer-wise fine-tuning top 1
