######################################################################
full model fine-tuning
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 243.45it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'eval_loss': 2.7424111366271973, 'eval_accuracy': 0.24333333333333335, 'eval_runtime': 50.4441, 'eval_samples_per_second': 29.736, 'eval_steps_per_second': 1.863, 'epoch': 1.0}
{'eval_loss': 2.7993152141571045, 'eval_accuracy': 0.24666666666666667, 'eval_runtime': 50.4906, 'eval_samples_per_second': 29.708, 'eval_steps_per_second': 1.862, 'epoch': 2.0}
{'loss': 2.8711, 'learning_rate': 2.5e-06, 'epoch': 2.78}
{'eval_loss': 2.696227788925171, 'eval_accuracy': 0.23733333333333334, 'eval_runtime': 50.5894, 'eval_samples_per_second': 29.65, 'eval_steps_per_second': 1.858, 'epoch': 3.0}
{'eval_loss': 2.9186928272247314, 'eval_accuracy': 0.24666666666666667, 'eval_runtime': 50.5729, 'eval_samples_per_second': 29.66, 'eval_steps_per_second': 1.859, 'epoch': 4.0}
{'eval_loss': 2.954315423965454, 'eval_accuracy': 0.24266666666666667, 'eval_runtime': 51.03, 'eval_samples_per_second': 29.394, 'eval_steps_per_second': 1.842, 'epoch': 5.0}
{'loss': 1.9675, 'learning_rate': 5e-06, 'epoch': 5.56}
{'eval_loss': 3.3303632736206055, 'eval_accuracy': 0.252, 'eval_runtime': 50.5094, 'eval_samples_per_second': 29.697, 'eval_steps_per_second': 1.861, 'epoch': 6.0}
{'eval_loss': 2.319282293319702, 'eval_accuracy': 0.226, 'eval_runtime': 50.7722, 'eval_samples_per_second': 29.544, 'eval_steps_per_second': 1.851, 'epoch': 7.0}
{'eval_loss': 2.612656354904175, 'eval_accuracy': 0.24666666666666667, 'eval_runtime': 50.421, 'eval_samples_per_second': 29.75, 'eval_steps_per_second': 1.864, 'epoch': 8.0}
{'loss': 1.8692, 'learning_rate': 7.5e-06, 'epoch': 8.33}
{'eval_loss': 3.9911367893218994, 'eval_accuracy': 0.18066666666666667, 'eval_runtime': 50.6621, 'eval_samples_per_second': 29.608, 'eval_steps_per_second': 1.855, 'epoch': 9.0}
{'eval_loss': 2.7504935264587402, 'eval_accuracy': 0.23266666666666666, 'eval_runtime': 50.4192, 'eval_samples_per_second': 29.751, 'eval_steps_per_second': 1.864, 'epoch': 10.0}
{'train_runtime': 5987.3455, 'train_samples_per_second': 9.602, 'train_steps_per_second': 0.301, 'train_loss': 2.1385623508029514, 'epoch': 10.0}
Total training time: 5987.37 seconds
######################################################################
layer-wise fine-tuning top 1
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 297.65it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('roberta.encoder.layer.0', 'roberta.encoder.layer.1', 'roberta.encoder.layer.2', 'roberta.encoder.layer.3', 'roberta.encoder.layer.4', 'roberta.encoder.layer.5', 'roberta.encoder.layer.6', 'roberta.encoder.layer.7', 'roberta.encoder.layer.8', 'roberta.encoder.layer.9', 'roberta.encoder.layer.10')
{'eval_loss': 2.7424111366271973, 'eval_accuracy': 0.24333333333333335, 'eval_runtime': 50.5287, 'eval_samples_per_second': 29.686, 'eval_steps_per_second': 1.86, 'epoch': 1.0}
{'eval_loss': 2.7993152141571045, 'eval_accuracy': 0.24666666666666667, 'eval_runtime': 50.7803, 'eval_samples_per_second': 29.539, 'eval_steps_per_second': 1.851, 'epoch': 2.0}
{'loss': 2.8711, 'learning_rate': 2.5e-06, 'epoch': 2.78}
{'eval_loss': 2.696227788925171, 'eval_accuracy': 0.23733333333333334, 'eval_runtime': 50.3835, 'eval_samples_per_second': 29.772, 'eval_steps_per_second': 1.866, 'epoch': 3.0}
{'eval_loss': 2.9186928272247314, 'eval_accuracy': 0.24666666666666667, 'eval_runtime': 50.4068, 'eval_samples_per_second': 29.758, 'eval_steps_per_second': 1.865, 'epoch': 4.0}
{'eval_loss': 2.954315423965454, 'eval_accuracy': 0.24266666666666667, 'eval_runtime': 51.2142, 'eval_samples_per_second': 29.289, 'eval_steps_per_second': 1.835, 'epoch': 5.0}
{'loss': 1.9675, 'learning_rate': 5e-06, 'epoch': 5.56}
{'eval_loss': 3.3303632736206055, 'eval_accuracy': 0.252, 'eval_runtime': 50.695, 'eval_samples_per_second': 29.589, 'eval_steps_per_second': 1.854, 'epoch': 6.0}
{'eval_loss': 2.319282293319702, 'eval_accuracy': 0.226, 'eval_runtime': 50.5016, 'eval_samples_per_second': 29.702, 'eval_steps_per_second': 1.861, 'epoch': 7.0}
{'eval_loss': 2.612656354904175, 'eval_accuracy': 0.24666666666666667, 'eval_runtime': 50.496, 'eval_samples_per_second': 29.705, 'eval_steps_per_second': 1.862, 'epoch': 8.0}
{'loss': 1.8692, 'learning_rate': 7.5e-06, 'epoch': 8.33}
{'eval_loss': 3.9911367893218994, 'eval_accuracy': 0.18066666666666667, 'eval_runtime': 50.8699, 'eval_samples_per_second': 29.487, 'eval_steps_per_second': 1.848, 'epoch': 9.0}
{'eval_loss': 2.7504935264587402, 'eval_accuracy': 0.23266666666666666, 'eval_runtime': 50.4616, 'eval_samples_per_second': 29.726, 'eval_steps_per_second': 1.863, 'epoch': 10.0}
{'train_runtime': 5993.1718, 'train_samples_per_second': 9.593, 'train_steps_per_second': 0.3, 'train_loss': 2.1385623508029514, 'epoch': 10.0}
Total training time: 5993.19 seconds
######################################################################
layer-wise fine-tuning top 2
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 318.51it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('roberta.encoder.layer.0', 'roberta.encoder.layer.1', 'roberta.encoder.layer.2', 'roberta.encoder.layer.3', 'roberta.encoder.layer.4', 'roberta.encoder.layer.5', 'roberta.encoder.layer.6', 'roberta.encoder.layer.7', 'roberta.encoder.layer.8', 'roberta.encoder.layer.9')
{'eval_loss': 2.7424111366271973, 'eval_accuracy': 0.24333333333333335, 'eval_runtime': 50.4566, 'eval_samples_per_second': 29.729, 'eval_steps_per_second': 1.863, 'epoch': 1.0}
{'eval_loss': 2.7993152141571045, 'eval_accuracy': 0.24666666666666667, 'eval_runtime': 50.5788, 'eval_samples_per_second': 29.657, 'eval_steps_per_second': 1.858, 'epoch': 2.0}
{'loss': 2.8711, 'learning_rate': 2.5e-06, 'epoch': 2.78}
{'eval_loss': 2.696227788925171, 'eval_accuracy': 0.23733333333333334, 'eval_runtime': 49.8991, 'eval_samples_per_second': 30.061, 'eval_steps_per_second': 1.884, 'epoch': 3.0}
{'eval_loss': 2.9186928272247314, 'eval_accuracy': 0.24666666666666667, 'eval_runtime': 49.838, 'eval_samples_per_second': 30.098, 'eval_steps_per_second': 1.886, 'epoch': 4.0}
{'eval_loss': 2.954315423965454, 'eval_accuracy': 0.24266666666666667, 'eval_runtime': 49.8846, 'eval_samples_per_second': 30.069, 'eval_steps_per_second': 1.884, 'epoch': 5.0}
{'loss': 1.9675, 'learning_rate': 5e-06, 'epoch': 5.56}
{'eval_loss': 3.3303632736206055, 'eval_accuracy': 0.252, 'eval_runtime': 49.8861, 'eval_samples_per_second': 30.068, 'eval_steps_per_second': 1.884, 'epoch': 6.0}
{'eval_loss': 2.319282293319702, 'eval_accuracy': 0.226, 'eval_runtime': 49.899, 'eval_samples_per_second': 30.061, 'eval_steps_per_second': 1.884, 'epoch': 7.0}
{'eval_loss': 2.612656354904175, 'eval_accuracy': 0.24666666666666667, 'eval_runtime': 49.9062, 'eval_samples_per_second': 30.056, 'eval_steps_per_second': 1.884, 'epoch': 8.0}
{'loss': 1.8692, 'learning_rate': 7.5e-06, 'epoch': 8.33}
{'eval_loss': 3.9911367893218994, 'eval_accuracy': 0.18066666666666667, 'eval_runtime': 49.8879, 'eval_samples_per_second': 30.067, 'eval_steps_per_second': 1.884, 'epoch': 9.0}
{'eval_loss': 2.7504935264587402, 'eval_accuracy': 0.23266666666666666, 'eval_runtime': 49.8612, 'eval_samples_per_second': 30.084, 'eval_steps_per_second': 1.885, 'epoch': 10.0}
{'train_runtime': 5915.9394, 'train_samples_per_second': 9.718, 'train_steps_per_second': 0.304, 'train_loss': 2.1385623508029514, 'epoch': 10.0}
Total training time: 5915.96 seconds
######################################################################
layer-wise fine-tuning top 3
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 190.68it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('roberta.encoder.layer.0', 'roberta.encoder.layer.1', 'roberta.encoder.layer.2', 'roberta.encoder.layer.3', 'roberta.encoder.layer.4', 'roberta.encoder.layer.5', 'roberta.encoder.layer.6', 'roberta.encoder.layer.7', 'roberta.encoder.layer.8')
{'eval_loss': 2.7424111366271973, 'eval_accuracy': 0.24333333333333335, 'eval_runtime': 49.8795, 'eval_samples_per_second': 30.072, 'eval_steps_per_second': 1.885, 'epoch': 1.0}
{'eval_loss': 2.7993152141571045, 'eval_accuracy': 0.24666666666666667, 'eval_runtime': 49.9391, 'eval_samples_per_second': 30.037, 'eval_steps_per_second': 1.882, 'epoch': 2.0}
{'loss': 2.8711, 'learning_rate': 2.5e-06, 'epoch': 2.78}
{'eval_loss': 2.696227788925171, 'eval_accuracy': 0.23733333333333334, 'eval_runtime': 49.8633, 'eval_samples_per_second': 30.082, 'eval_steps_per_second': 1.885, 'epoch': 3.0}
{'eval_loss': 2.9186928272247314, 'eval_accuracy': 0.24666666666666667, 'eval_runtime': 49.9192, 'eval_samples_per_second': 30.049, 'eval_steps_per_second': 1.883, 'epoch': 4.0}
{'eval_loss': 2.954315423965454, 'eval_accuracy': 0.24266666666666667, 'eval_runtime': 49.8075, 'eval_samples_per_second': 30.116, 'eval_steps_per_second': 1.887, 'epoch': 5.0}
{'loss': 1.9675, 'learning_rate': 5e-06, 'epoch': 5.56}
{'eval_loss': 3.3303632736206055, 'eval_accuracy': 0.252, 'eval_runtime': 49.858, 'eval_samples_per_second': 30.085, 'eval_steps_per_second': 1.885, 'epoch': 6.0}
{'eval_loss': 2.319282293319702, 'eval_accuracy': 0.226, 'eval_runtime': 49.9186, 'eval_samples_per_second': 30.049, 'eval_steps_per_second': 1.883, 'epoch': 7.0}
{'eval_loss': 2.612656354904175, 'eval_accuracy': 0.24666666666666667, 'eval_runtime': 49.9347, 'eval_samples_per_second': 30.039, 'eval_steps_per_second': 1.882, 'epoch': 8.0}
{'loss': 1.8692, 'learning_rate': 7.5e-06, 'epoch': 8.33}
{'eval_loss': 3.9911367893218994, 'eval_accuracy': 0.18066666666666667, 'eval_runtime': 49.8745, 'eval_samples_per_second': 30.076, 'eval_steps_per_second': 1.885, 'epoch': 9.0}
{'eval_loss': 2.7504935264587402, 'eval_accuracy': 0.23266666666666666, 'eval_runtime': 49.9089, 'eval_samples_per_second': 30.055, 'eval_steps_per_second': 1.883, 'epoch': 10.0}
{'train_runtime': 5905.8369, 'train_samples_per_second': 9.734, 'train_steps_per_second': 0.305, 'train_loss': 2.1385623508029514, 'epoch': 10.0}
Total training time: 5905.86 seconds
######################################################################
layer-wise fine-tuning top 4
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 348.93it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('roberta.encoder.layer.0', 'roberta.encoder.layer.1', 'roberta.encoder.layer.2', 'roberta.encoder.layer.3', 'roberta.encoder.layer.4', 'roberta.encoder.layer.5', 'roberta.encoder.layer.6', 'roberta.encoder.layer.8')
{'eval_loss': 2.7424111366271973, 'eval_accuracy': 0.24333333333333335, 'eval_runtime': 49.8237, 'eval_samples_per_second': 30.106, 'eval_steps_per_second': 1.887, 'epoch': 1.0}
{'eval_loss': 2.7993152141571045, 'eval_accuracy': 0.24666666666666667, 'eval_runtime': 49.7882, 'eval_samples_per_second': 30.128, 'eval_steps_per_second': 1.888, 'epoch': 2.0}
{'loss': 2.8711, 'learning_rate': 2.5e-06, 'epoch': 2.78}
{'eval_loss': 2.696227788925171, 'eval_accuracy': 0.23733333333333334, 'eval_runtime': 49.8078, 'eval_samples_per_second': 30.116, 'eval_steps_per_second': 1.887, 'epoch': 3.0}
{'eval_loss': 2.9186928272247314, 'eval_accuracy': 0.24666666666666667, 'eval_runtime': 49.8567, 'eval_samples_per_second': 30.086, 'eval_steps_per_second': 1.885, 'epoch': 4.0}
{'eval_loss': 2.954315423965454, 'eval_accuracy': 0.24266666666666667, 'eval_runtime': 49.7983, 'eval_samples_per_second': 30.122, 'eval_steps_per_second': 1.888, 'epoch': 5.0}
{'loss': 1.9675, 'learning_rate': 5e-06, 'epoch': 5.56}
{'eval_loss': 3.3303632736206055, 'eval_accuracy': 0.252, 'eval_runtime': 49.8785, 'eval_samples_per_second': 30.073, 'eval_steps_per_second': 1.885, 'epoch': 6.0}
{'eval_loss': 2.319282293319702, 'eval_accuracy': 0.226, 'eval_runtime': 49.8471, 'eval_samples_per_second': 30.092, 'eval_steps_per_second': 1.886, 'epoch': 7.0}
{'eval_loss': 2.612656354904175, 'eval_accuracy': 0.24666666666666667, 'eval_runtime': 49.8538, 'eval_samples_per_second': 30.088, 'eval_steps_per_second': 1.886, 'epoch': 8.0}
{'loss': 1.8692, 'learning_rate': 7.5e-06, 'epoch': 8.33}
{'eval_loss': 3.9911367893218994, 'eval_accuracy': 0.18066666666666667, 'eval_runtime': 49.8266, 'eval_samples_per_second': 30.104, 'eval_steps_per_second': 1.887, 'epoch': 9.0}
{'eval_loss': 2.7504935264587402, 'eval_accuracy': 0.23266666666666666, 'eval_runtime': 49.88, 'eval_samples_per_second': 30.072, 'eval_steps_per_second': 1.885, 'epoch': 10.0}
{'train_runtime': 5898.0626, 'train_samples_per_second': 9.747, 'train_steps_per_second': 0.305, 'train_loss': 2.1385623508029514, 'epoch': 10.0}
Total training time: 5898.10 seconds
######################################################################
layer-wise fine-tuning top 5
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 304.18it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('roberta.encoder.layer.0', 'roberta.encoder.layer.1', 'roberta.encoder.layer.2', 'roberta.encoder.layer.4', 'roberta.encoder.layer.5', 'roberta.encoder.layer.6', 'roberta.encoder.layer.8')
{'eval_loss': 2.7424111366271973, 'eval_accuracy': 0.24333333333333335, 'eval_runtime': 49.7292, 'eval_samples_per_second': 30.163, 'eval_steps_per_second': 1.89, 'epoch': 1.0}
{'eval_loss': 2.7993152141571045, 'eval_accuracy': 0.24666666666666667, 'eval_runtime': 49.5858, 'eval_samples_per_second': 30.251, 'eval_steps_per_second': 1.896, 'epoch': 2.0}
{'loss': 2.8711, 'learning_rate': 2.5e-06, 'epoch': 2.78}
{'eval_loss': 2.696227788925171, 'eval_accuracy': 0.23733333333333334, 'eval_runtime': 49.5729, 'eval_samples_per_second': 30.258, 'eval_steps_per_second': 1.896, 'epoch': 3.0}
{'eval_loss': 2.9186928272247314, 'eval_accuracy': 0.24666666666666667, 'eval_runtime': 49.4833, 'eval_samples_per_second': 30.313, 'eval_steps_per_second': 1.9, 'epoch': 4.0}
{'eval_loss': 2.954315423965454, 'eval_accuracy': 0.24266666666666667, 'eval_runtime': 49.5632, 'eval_samples_per_second': 30.264, 'eval_steps_per_second': 1.897, 'epoch': 5.0}
{'loss': 1.9675, 'learning_rate': 5e-06, 'epoch': 5.56}
{'eval_loss': 3.3303632736206055, 'eval_accuracy': 0.252, 'eval_runtime': 49.6723, 'eval_samples_per_second': 30.198, 'eval_steps_per_second': 1.892, 'epoch': 6.0}
{'eval_loss': 2.319282293319702, 'eval_accuracy': 0.226, 'eval_runtime': 49.6824, 'eval_samples_per_second': 30.192, 'eval_steps_per_second': 1.892, 'epoch': 7.0}
{'eval_loss': 2.612656354904175, 'eval_accuracy': 0.24666666666666667, 'eval_runtime': 49.6814, 'eval_samples_per_second': 30.192, 'eval_steps_per_second': 1.892, 'epoch': 8.0}
{'loss': 1.8692, 'learning_rate': 7.5e-06, 'epoch': 8.33}
{'eval_loss': 3.9911367893218994, 'eval_accuracy': 0.18066666666666667, 'eval_runtime': 49.653, 'eval_samples_per_second': 30.21, 'eval_steps_per_second': 1.893, 'epoch': 9.0}
{'eval_loss': 2.7504935264587402, 'eval_accuracy': 0.23266666666666666, 'eval_runtime': 49.6756, 'eval_samples_per_second': 30.196, 'eval_steps_per_second': 1.892, 'epoch': 10.0}
{'train_runtime': 5894.3453, 'train_samples_per_second': 9.753, 'train_steps_per_second': 0.305, 'train_loss': 2.1385623508029514, 'epoch': 10.0}
Total training time: 5894.36 seconds
######################################################################
finished
