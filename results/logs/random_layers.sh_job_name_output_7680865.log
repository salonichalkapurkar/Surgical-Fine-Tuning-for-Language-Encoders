######################################################################
Random layers finetuning for 3 layers
######################################################################
######################################################################
layer-wise fine-tuning random 3 - rte
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 64.07it/s]
/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.4', 'bert.encoder.layer.5', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.9', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 0.8313344120979309, 'eval_accuracy': 0.5270758122743683, 'eval_runtime': 8.6295, 'eval_samples_per_second': 32.099, 'eval_steps_per_second': 2.086, 'epoch': 1.0}
{'eval_loss': 0.8060948252677917, 'eval_accuracy': 0.5270758122743683, 'eval_runtime': 8.7535, 'eval_samples_per_second': 31.644, 'eval_steps_per_second': 2.056, 'epoch': 2.0}
{'eval_loss': 0.7659504413604736, 'eval_accuracy': 0.5270758122743683, 'eval_runtime': 8.5826, 'eval_samples_per_second': 32.274, 'eval_steps_per_second': 2.097, 'epoch': 3.0}
{'eval_loss': 0.7185186147689819, 'eval_accuracy': 0.5270758122743683, 'eval_runtime': 8.549, 'eval_samples_per_second': 32.401, 'eval_steps_per_second': 2.106, 'epoch': 4.0}
{'eval_loss': 0.6950769424438477, 'eval_accuracy': 0.5523465703971119, 'eval_runtime': 8.4937, 'eval_samples_per_second': 32.613, 'eval_steps_per_second': 2.119, 'epoch': 5.0}
{'eval_loss': 0.6900577545166016, 'eval_accuracy': 0.5523465703971119, 'eval_runtime': 8.6584, 'eval_samples_per_second': 31.992, 'eval_steps_per_second': 2.079, 'epoch': 6.0}
{'loss': 0.7823, 'learning_rate': 2.5e-06, 'epoch': 6.41}
{'eval_loss': 0.6858910918235779, 'eval_accuracy': 0.5703971119133574, 'eval_runtime': 8.5954, 'eval_samples_per_second': 32.226, 'eval_steps_per_second': 2.094, 'epoch': 7.0}
{'eval_loss': 0.682463526725769, 'eval_accuracy': 0.5812274368231047, 'eval_runtime': 8.542, 'eval_samples_per_second': 32.428, 'eval_steps_per_second': 2.107, 'epoch': 8.0}
{'eval_loss': 0.680935800075531, 'eval_accuracy': 0.5703971119133574, 'eval_runtime': 8.5023, 'eval_samples_per_second': 32.579, 'eval_steps_per_second': 2.117, 'epoch': 9.0}
{'eval_loss': 0.6782819032669067, 'eval_accuracy': 0.5703971119133574, 'eval_runtime': 8.5986, 'eval_samples_per_second': 32.215, 'eval_steps_per_second': 2.093, 'epoch': 10.0}
{'train_runtime': 1930.3642, 'train_samples_per_second': 12.899, 'train_steps_per_second': 0.404, 'train_loss': 0.7389613420535357, 'epoch': 10.0}
Total training time: 1930.39 seconds
######################################################################
layer-wise fine-tuning random 3 - qnli
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:00<00:01,  1.85it/s]100%|██████████| 3/3 [00:00<00:00,  4.99it/s]
/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.2', 'bert.encoder.layer.4', 'bert.encoder.layer.5', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.11')
{'loss': 0.7187, 'learning_rate': 2.5e-06, 'epoch': 0.15}
{'loss': 0.5283, 'learning_rate': 5e-06, 'epoch': 0.31}
{'loss': 0.4671, 'learning_rate': 7.5e-06, 'epoch': 0.46}
{'loss': 0.4263, 'learning_rate': 1e-05, 'epoch': 0.61}
{'loss': 0.4092, 'learning_rate': 1.25e-05, 'epoch': 0.76}
{'loss': 0.3904, 'learning_rate': 1.5e-05, 'epoch': 0.92}
{'eval_loss': 0.3128211498260498, 'eval_accuracy': 0.8654585392641406, 'eval_runtime': 167.7358, 'eval_samples_per_second': 32.569, 'eval_steps_per_second': 2.039, 'epoch': 1.0}
{'loss': 0.3602, 'learning_rate': 1.75e-05, 'epoch': 1.07}
{'loss': 0.3401, 'learning_rate': 2e-05, 'epoch': 1.22}
{'loss': 0.3333, 'learning_rate': 2.25e-05, 'epoch': 1.37}
{'loss': 0.3288, 'learning_rate': 2.5e-05, 'epoch': 1.53}
{'loss': 0.3295, 'learning_rate': 2.7500000000000004e-05, 'epoch': 1.68}
{'loss': 0.3204, 'learning_rate': 3e-05, 'epoch': 1.83}
{'loss': 0.3064, 'learning_rate': 3.2500000000000004e-05, 'epoch': 1.99}
{'eval_loss': 0.2760688066482544, 'eval_accuracy': 0.8866922936115688, 'eval_runtime': 168.5742, 'eval_samples_per_second': 32.407, 'eval_steps_per_second': 2.029, 'epoch': 2.0}
{'loss': 0.251, 'learning_rate': 3.5e-05, 'epoch': 2.14}
{'loss': 0.243, 'learning_rate': 3.7500000000000003e-05, 'epoch': 2.29}
{'loss': 0.2441, 'learning_rate': 4e-05, 'epoch': 2.44}
{'loss': 0.2495, 'learning_rate': 4.25e-05, 'epoch': 2.6}
{'loss': 0.254, 'learning_rate': 4.5e-05, 'epoch': 2.75}
{'loss': 0.253, 'learning_rate': 4.75e-05, 'epoch': 2.9}
{'eval_loss': 0.2851700186729431, 'eval_accuracy': 0.8930990298370859, 'eval_runtime': 169.1411, 'eval_samples_per_second': 32.298, 'eval_steps_per_second': 2.022, 'epoch': 3.0}
{'loss': 0.2207, 'learning_rate': 5e-05, 'epoch': 3.05}
{'loss': 0.1717, 'learning_rate': 4.8900131984161904e-05, 'epoch': 3.21}
{'loss': 0.161, 'learning_rate': 4.7800263968323806e-05, 'epoch': 3.36}
{'loss': 0.1722, 'learning_rate': 4.67003959524857e-05, 'epoch': 3.51}
{'loss': 0.1739, 'learning_rate': 4.56005279366476e-05, 'epoch': 3.67}
{'loss': 0.1792, 'learning_rate': 4.4500659920809504e-05, 'epoch': 3.82}
{'loss': 0.1681, 'learning_rate': 4.3400791904971405e-05, 'epoch': 3.97}
{'eval_loss': 0.31428343057632446, 'eval_accuracy': 0.8903532857404357, 'eval_runtime': 168.2411, 'eval_samples_per_second': 32.471, 'eval_steps_per_second': 2.033, 'epoch': 4.0}
{'loss': 0.1144, 'learning_rate': 4.230092388913331e-05, 'epoch': 4.12}
{'loss': 0.1081, 'learning_rate': 4.120105587329521e-05, 'epoch': 4.28}
{'loss': 0.1056, 'learning_rate': 4.01011878574571e-05, 'epoch': 4.43}
{'loss': 0.1056, 'learning_rate': 3.9001319841619005e-05, 'epoch': 4.58}
{'loss': 0.1147, 'learning_rate': 3.7901451825780906e-05, 'epoch': 4.73}
{'loss': 0.112, 'learning_rate': 3.680158380994281e-05, 'epoch': 4.89}
{'eval_loss': 0.39737239480018616, 'eval_accuracy': 0.8866922936115688, 'eval_runtime': 169.6363, 'eval_samples_per_second': 32.204, 'eval_steps_per_second': 2.016, 'epoch': 5.0}
{'loss': 0.0985, 'learning_rate': 3.570171579410471e-05, 'epoch': 5.04}
{'loss': 0.0637, 'learning_rate': 3.460184777826661e-05, 'epoch': 5.19}
{'loss': 0.0738, 'learning_rate': 3.3501979762428506e-05, 'epoch': 5.35}
{'loss': 0.071, 'learning_rate': 3.2402111746590414e-05, 'epoch': 5.5}
{'loss': 0.0756, 'learning_rate': 3.130224373075231e-05, 'epoch': 5.65}
{'loss': 0.0766, 'learning_rate': 3.020237571491421e-05, 'epoch': 5.8}
{'loss': 0.0737, 'learning_rate': 2.9102507699076116e-05, 'epoch': 5.96}
{'eval_loss': 0.4737740457057953, 'eval_accuracy': 0.8885227896760022, 'eval_runtime': 169.3401, 'eval_samples_per_second': 32.261, 'eval_steps_per_second': 2.02, 'epoch': 6.0}
{'loss': 0.0577, 'learning_rate': 2.8002639683238014e-05, 'epoch': 6.11}
{'loss': 0.0457, 'learning_rate': 2.6902771667399912e-05, 'epoch': 6.26}
{'loss': 0.0472, 'learning_rate': 2.5802903651561817e-05, 'epoch': 6.42}
{'loss': 0.0513, 'learning_rate': 2.4703035635723715e-05, 'epoch': 6.57}
{'loss': 0.0501, 'learning_rate': 2.3603167619885617e-05, 'epoch': 6.72}
{'loss': 0.0502, 'learning_rate': 2.2503299604047515e-05, 'epoch': 6.87}
{'eval_loss': 0.50982266664505, 'eval_accuracy': 0.8912685337726524, 'eval_runtime': 169.3666, 'eval_samples_per_second': 32.255, 'eval_steps_per_second': 2.019, 'epoch': 7.0}
{'loss': 0.0492, 'learning_rate': 2.1403431588209417e-05, 'epoch': 7.03}
{'loss': 0.0293, 'learning_rate': 2.030356357237132e-05, 'epoch': 7.18}
{'loss': 0.0364, 'learning_rate': 1.9203695556533217e-05, 'epoch': 7.33}
{'loss': 0.0325, 'learning_rate': 1.8103827540695118e-05, 'epoch': 7.48}
{'loss': 0.0325, 'learning_rate': 1.700395952485702e-05, 'epoch': 7.64}
{'loss': 0.0349, 'learning_rate': 1.5904091509018918e-05, 'epoch': 7.79}
{'loss': 0.0337, 'learning_rate': 1.480422349318082e-05, 'epoch': 7.94}
{'eval_loss': 0.6568050980567932, 'eval_accuracy': 0.8870583928244554, 'eval_runtime': 170.2255, 'eval_samples_per_second': 32.093, 'eval_steps_per_second': 2.009, 'epoch': 8.0}
{'loss': 0.0286, 'learning_rate': 1.370435547734272e-05, 'epoch': 8.1}
{'loss': 0.0251, 'learning_rate': 1.260448746150462e-05, 'epoch': 8.25}
{'loss': 0.0229, 'learning_rate': 1.1504619445666521e-05, 'epoch': 8.4}
{'loss': 0.0172, 'learning_rate': 1.0404751429828421e-05, 'epoch': 8.55}
{'loss': 0.0246, 'learning_rate': 9.30488341399032e-06, 'epoch': 8.71}
{'loss': 0.0212, 'learning_rate': 8.205015398152222e-06, 'epoch': 8.86}
{'eval_loss': 0.7013323307037354, 'eval_accuracy': 0.8894380377082189, 'eval_runtime': 169.6796, 'eval_samples_per_second': 32.196, 'eval_steps_per_second': 2.016, 'epoch': 9.0}
{'loss': 0.0203, 'learning_rate': 7.105147382314123e-06, 'epoch': 9.01}
{'loss': 0.0173, 'learning_rate': 6.005279366476023e-06, 'epoch': 9.16}
{'loss': 0.013, 'learning_rate': 4.905411350637924e-06, 'epoch': 9.32}
{'loss': 0.0171, 'learning_rate': 3.805543334799824e-06, 'epoch': 9.47}
{'loss': 0.0147, 'learning_rate': 2.7056753189617244e-06, 'epoch': 9.62}
{'loss': 0.0145, 'learning_rate': 1.6058073031236254e-06, 'epoch': 9.78}
{'loss': 0.0136, 'learning_rate': 5.059392872855257e-07, 'epoch': 9.93}
{'eval_loss': 0.7170089483261108, 'eval_accuracy': 0.8890719384953323, 'eval_runtime': 168.7236, 'eval_samples_per_second': 32.378, 'eval_steps_per_second': 2.027, 'epoch': 10.0}
{'train_runtime': 75779.0379, 'train_samples_per_second': 13.822, 'train_steps_per_second': 0.432, 'train_loss': 0.15279868526849186, 'epoch': 10.0}
Total training time: 75779.06 seconds
######################################################################
layer-wise fine-tuning random 3 - cola
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 108.20it/s]
Map:   0%|          | 0/1043 [00:00<?, ? examples/s]Map:  96%|█████████▌| 1000/1043 [00:00<00:00, 2982.96 examples/s]                                                                 /home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.4', 'bert.encoder.layer.5', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 0.7978261709213257, 'eval_matthews_correlation': 0.03496482977659122, 'eval_runtime': 32.1324, 'eval_samples_per_second': 32.459, 'eval_steps_per_second': 2.054, 'epoch': 1.0}
{'loss': 0.8127, 'learning_rate': 2.5e-06, 'epoch': 1.87}
{'eval_loss': 0.5911725759506226, 'eval_matthews_correlation': 0.0, 'eval_runtime': 32.1585, 'eval_samples_per_second': 32.433, 'eval_steps_per_second': 2.052, 'epoch': 2.0}
{'eval_loss': 0.539722740650177, 'eval_matthews_correlation': 0.24490515463628276, 'eval_runtime': 32.1603, 'eval_samples_per_second': 32.431, 'eval_steps_per_second': 2.052, 'epoch': 3.0}
{'loss': 0.5439, 'learning_rate': 5e-06, 'epoch': 3.74}
{'eval_loss': 0.5393992066383362, 'eval_matthews_correlation': 0.37671460599948314, 'eval_runtime': 32.1279, 'eval_samples_per_second': 32.464, 'eval_steps_per_second': 2.054, 'epoch': 4.0}
{'eval_loss': 0.5155189633369446, 'eval_matthews_correlation': 0.4216692731197749, 'eval_runtime': 32.1317, 'eval_samples_per_second': 32.46, 'eval_steps_per_second': 2.054, 'epoch': 5.0}
{'loss': 0.4418, 'learning_rate': 7.5e-06, 'epoch': 5.61}
{'eval_loss': 0.49007487297058105, 'eval_matthews_correlation': 0.4890316656219303, 'eval_runtime': 32.1605, 'eval_samples_per_second': 32.431, 'eval_steps_per_second': 2.052, 'epoch': 6.0}
{'eval_loss': 0.506659984588623, 'eval_matthews_correlation': 0.5206360818297229, 'eval_runtime': 32.1249, 'eval_samples_per_second': 32.467, 'eval_steps_per_second': 2.054, 'epoch': 7.0}
{'loss': 0.3611, 'learning_rate': 1e-05, 'epoch': 7.48}
{'eval_loss': 0.5160267949104309, 'eval_matthews_correlation': 0.5206048793014052, 'eval_runtime': 31.9483, 'eval_samples_per_second': 32.646, 'eval_steps_per_second': 2.066, 'epoch': 8.0}
{'eval_loss': 0.5585142970085144, 'eval_matthews_correlation': 0.5073136963601746, 'eval_runtime': 32.2111, 'eval_samples_per_second': 32.38, 'eval_steps_per_second': 2.049, 'epoch': 9.0}
{'loss': 0.2757, 'learning_rate': 1.25e-05, 'epoch': 9.35}
{'eval_loss': 0.5659036636352539, 'eval_matthews_correlation': 0.5522963379760243, 'eval_runtime': 32.2169, 'eval_samples_per_second': 32.374, 'eval_steps_per_second': 2.049, 'epoch': 9.98}
{'train_runtime': 6413.7953, 'train_samples_per_second': 13.332, 'train_steps_per_second': 0.416, 'train_loss': 0.47160597526178827, 'epoch': 9.98}
Total training time: 6413.81 seconds
######################################################################
layer-wise fine-tuning random 3 - sst2
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
Downloading readme:   0%|          | 0.00/27.9k [00:00<?, ?B/s]Downloading readme: 100%|██████████| 27.9k/27.9k [00:00<00:00, 13.5MB/s]
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:00<00:00,  8.10it/s]100%|██████████| 3/3 [00:00<00:00, 21.87it/s]
/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.1', 'bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.4', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9')
{'loss': 0.7713, 'learning_rate': 2.5e-06, 'epoch': 0.24}
{'loss': 0.6101, 'learning_rate': 5e-06, 'epoch': 0.48}
{'loss': 0.3661, 'learning_rate': 7.5e-06, 'epoch': 0.71}
{'loss': 0.309, 'learning_rate': 1e-05, 'epoch': 0.95}
{'eval_loss': 0.2673369348049164, 'eval_accuracy': 0.8818807339449541, 'eval_runtime': 26.8518, 'eval_samples_per_second': 32.475, 'eval_steps_per_second': 2.048, 'epoch': 1.0}
{'loss': 0.2716, 'learning_rate': 1.25e-05, 'epoch': 1.19}
{'loss': 0.2511, 'learning_rate': 1.5e-05, 'epoch': 1.43}
{'loss': 0.2291, 'learning_rate': 1.75e-05, 'epoch': 1.66}
{'loss': 0.2238, 'learning_rate': 2e-05, 'epoch': 1.9}
{'eval_loss': 0.243280827999115, 'eval_accuracy': 0.8990825688073395, 'eval_runtime': 26.836, 'eval_samples_per_second': 32.494, 'eval_steps_per_second': 2.049, 'epoch': 2.0}
{'loss': 0.1862, 'learning_rate': 2.25e-05, 'epoch': 2.14}
{'loss': 0.1681, 'learning_rate': 2.5e-05, 'epoch': 2.38}
{'loss': 0.1697, 'learning_rate': 2.7500000000000004e-05, 'epoch': 2.61}
{'loss': 0.1622, 'learning_rate': 3e-05, 'epoch': 2.85}
{'eval_loss': 0.24671106040477753, 'eval_accuracy': 0.9059633027522935, 'eval_runtime': 26.6822, 'eval_samples_per_second': 32.681, 'eval_steps_per_second': 2.061, 'epoch': 3.0}
{'loss': 0.146, 'learning_rate': 3.2500000000000004e-05, 'epoch': 3.09}
{'loss': 0.1315, 'learning_rate': 3.5e-05, 'epoch': 3.33}
{'loss': 0.1271, 'learning_rate': 3.7500000000000003e-05, 'epoch': 3.56}
{'loss': 0.1303, 'learning_rate': 4e-05, 'epoch': 3.8}
{'eval_loss': 0.30428817868232727, 'eval_accuracy': 0.9036697247706422, 'eval_runtime': 26.9259, 'eval_samples_per_second': 32.385, 'eval_steps_per_second': 2.043, 'epoch': 4.0}
{'loss': 0.1238, 'learning_rate': 4.25e-05, 'epoch': 4.04}
{'loss': 0.0992, 'learning_rate': 4.5e-05, 'epoch': 4.28}
{'loss': 0.1023, 'learning_rate': 4.75e-05, 'epoch': 4.51}
{'loss': 0.1062, 'learning_rate': 5e-05, 'epoch': 4.75}
{'loss': 0.1053, 'learning_rate': 4.7737556561085976e-05, 'epoch': 4.99}
{'eval_loss': 0.31765392422676086, 'eval_accuracy': 0.9002293577981652, 'eval_runtime': 26.8167, 'eval_samples_per_second': 32.517, 'eval_steps_per_second': 2.051, 'epoch': 5.0}
{'loss': 0.0788, 'learning_rate': 4.547511312217195e-05, 'epoch': 5.23}
{'loss': 0.0732, 'learning_rate': 4.321266968325792e-05, 'epoch': 5.46}
{'loss': 0.0829, 'learning_rate': 4.095022624434389e-05, 'epoch': 5.7}
{'loss': 0.0814, 'learning_rate': 3.868778280542987e-05, 'epoch': 5.94}
{'eval_loss': 0.2941621243953705, 'eval_accuracy': 0.911697247706422, 'eval_runtime': 26.8756, 'eval_samples_per_second': 32.446, 'eval_steps_per_second': 2.046, 'epoch': 6.0}
{'loss': 0.0616, 'learning_rate': 3.642533936651584e-05, 'epoch': 6.18}
{'loss': 0.056, 'learning_rate': 3.416289592760181e-05, 'epoch': 6.41}
{'loss': 0.0575, 'learning_rate': 3.1900452488687784e-05, 'epoch': 6.65}
{'loss': 0.0632, 'learning_rate': 2.9638009049773758e-05, 'epoch': 6.89}
{'eval_loss': 0.3783077895641327, 'eval_accuracy': 0.9071100917431193, 'eval_runtime': 26.7864, 'eval_samples_per_second': 32.554, 'eval_steps_per_second': 2.053, 'epoch': 7.0}
{'loss': 0.0465, 'learning_rate': 2.737556561085973e-05, 'epoch': 7.13}
{'loss': 0.0403, 'learning_rate': 2.51131221719457e-05, 'epoch': 7.36}
{'loss': 0.0445, 'learning_rate': 2.2850678733031675e-05, 'epoch': 7.6}
{'loss': 0.0481, 'learning_rate': 2.058823529411765e-05, 'epoch': 7.84}
{'eval_loss': 0.4122551679611206, 'eval_accuracy': 0.9071100917431193, 'eval_runtime': 27.7151, 'eval_samples_per_second': 31.463, 'eval_steps_per_second': 1.984, 'epoch': 8.0}
{'loss': 0.034, 'learning_rate': 1.832579185520362e-05, 'epoch': 8.08}
{'loss': 0.0294, 'learning_rate': 1.6063348416289596e-05, 'epoch': 8.31}
{'loss': 0.0289, 'learning_rate': 1.3800904977375568e-05, 'epoch': 8.55}
{'loss': 0.0347, 'learning_rate': 1.153846153846154e-05, 'epoch': 8.79}
{'eval_loss': 0.42441070079803467, 'eval_accuracy': 0.9094036697247706, 'eval_runtime': 27.0468, 'eval_samples_per_second': 32.24, 'eval_steps_per_second': 2.034, 'epoch': 9.0}
{'loss': 0.0333, 'learning_rate': 9.276018099547511e-06, 'epoch': 9.03}
{'loss': 0.0199, 'learning_rate': 7.013574660633485e-06, 'epoch': 9.26}
{'loss': 0.0231, 'learning_rate': 4.751131221719457e-06, 'epoch': 9.5}
{'loss': 0.0245, 'learning_rate': 2.48868778280543e-06, 'epoch': 9.74}
{'loss': 0.0213, 'learning_rate': 2.2624434389140275e-07, 'epoch': 9.98}
{'eval_loss': 0.4868409037590027, 'eval_accuracy': 0.908256880733945, 'eval_runtime': 27.0756, 'eval_samples_per_second': 32.206, 'eval_steps_per_second': 2.031, 'epoch': 10.0}
{'train_runtime': 45352.875, 'train_samples_per_second': 14.85, 'train_steps_per_second': 0.464, 'train_loss': 0.13717290260729484, 'epoch': 10.0}
Traceback (most recent call last):
  File "code/fine_tuner.py", line 150, in <module>
    main(args)
  File "code/fine_tuner.py", line 123, in main
    trainer.train()
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 1633, in train
    return inner_training_loop(
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 2040, in _inner_training_loop
    checkpoints_sorted = self._sorted_checkpoints(use_mtime=False, output_dir=run_dir)
  File "/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/trainer.py", line 2865, in _sorted_checkpoints
    best_model_index = checkpoints_sorted.index(str(Path(self.state.best_model_checkpoint)))
ValueError: 'checkpoints/checkpoint-4210' is not in list
######################################################################
layer-wise fine-tuning random 3 - mrpc
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 71.60it/s]
Map:   0%|          | 0/408 [00:00<?, ? examples/s]Map: 100%|██████████| 408/408 [00:00<00:00, 2149.95 examples/s]                                                               /home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.1', 'bert.encoder.layer.2', 'bert.encoder.layer.4', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 1.0825035572052002, 'eval_accuracy': 0.3161764705882353, 'eval_runtime': 12.8358, 'eval_samples_per_second': 31.786, 'eval_steps_per_second': 2.026, 'epoch': 1.0}
{'eval_loss': 0.9672947525978088, 'eval_accuracy': 0.3161764705882353, 'eval_runtime': 12.7552, 'eval_samples_per_second': 31.987, 'eval_steps_per_second': 2.038, 'epoch': 2.0}
{'eval_loss': 0.6190358996391296, 'eval_accuracy': 0.678921568627451, 'eval_runtime': 12.6428, 'eval_samples_per_second': 32.271, 'eval_steps_per_second': 2.057, 'epoch': 3.0}
{'eval_loss': 0.597025990486145, 'eval_accuracy': 0.678921568627451, 'eval_runtime': 12.6892, 'eval_samples_per_second': 32.153, 'eval_steps_per_second': 2.049, 'epoch': 4.0}
{'loss': 0.8561, 'learning_rate': 2.5e-06, 'epoch': 4.35}
{'eval_loss': 0.5620602965354919, 'eval_accuracy': 0.6887254901960784, 'eval_runtime': 12.735, 'eval_samples_per_second': 32.038, 'eval_steps_per_second': 2.042, 'epoch': 5.0}
{'eval_loss': 0.5357493162155151, 'eval_accuracy': 0.7352941176470589, 'eval_runtime': 12.8347, 'eval_samples_per_second': 31.789, 'eval_steps_per_second': 2.026, 'epoch': 6.0}
{'eval_loss': 0.5222771763801575, 'eval_accuracy': 0.7450980392156863, 'eval_runtime': 12.703, 'eval_samples_per_second': 32.118, 'eval_steps_per_second': 2.047, 'epoch': 7.0}
{'eval_loss': 0.5113269686698914, 'eval_accuracy': 0.7573529411764706, 'eval_runtime': 12.6906, 'eval_samples_per_second': 32.15, 'eval_steps_per_second': 2.049, 'epoch': 8.0}
{'loss': 0.5289, 'learning_rate': 5e-06, 'epoch': 8.7}
{'eval_loss': 0.49242010712623596, 'eval_accuracy': 0.7769607843137255, 'eval_runtime': 12.7845, 'eval_samples_per_second': 31.914, 'eval_steps_per_second': 2.034, 'epoch': 9.0}
{'eval_loss': 0.48529523611068726, 'eval_accuracy': 0.7794117647058824, 'eval_runtime': 12.7704, 'eval_samples_per_second': 31.949, 'eval_steps_per_second': 2.036, 'epoch': 10.0}
{'train_runtime': 2850.2662, 'train_samples_per_second': 12.869, 'train_steps_per_second': 0.403, 'train_loss': 0.6616325909158458, 'epoch': 10.0}
Total training time: 2850.28 seconds
######################################################################
layer-wise fine-tuning random 3 - qqp
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:01<00:03,  1.71s/it] 67%|██████▋   | 2/3 [00:01<00:00,  1.26it/s]100%|██████████| 3/3 [00:03<00:00,  1.22s/it]100%|██████████| 3/3 [00:03<00:00,  1.20s/it]
Map:   0%|          | 0/40430 [00:00<?, ? examples/s]Map:   2%|▏         | 1000/40430 [00:00<00:22, 1734.12 examples/s]Map:   5%|▍         | 2000/40430 [00:00<00:15, 2540.12 examples/s]Map:   7%|▋         | 3000/40430 [00:01<00:12, 3026.89 examples/s]Map:  10%|▉         | 4000/40430 [00:01<00:12, 2954.82 examples/s]Map:  12%|█▏        | 5000/40430 [00:01<00:10, 3267.51 examples/s]Map:  15%|█▍        | 6000/40430 [00:01<00:09, 3473.28 examples/s]Map:  17%|█▋        | 7000/40430 [00:02<00:09, 3636.68 examples/s]Map:  20%|█▉        | 8000/40430 [00:02<00:08, 3724.54 examples/s]Map:  22%|██▏       | 9000/40430 [00:02<00:08, 3787.59 examples/s]Map:  25%|██▍       | 10000/40430 [00:03<00:09, 3197.24 examples/s]Map:  27%|██▋       | 11000/40430 [00:03<00:08, 3366.45 examples/s]Map:  30%|██▉       | 12000/40430 [00:03<00:08, 3401.01 examples/s]Map:  32%|███▏      | 13000/40430 [00:03<00:07, 3446.76 examples/s]Map:  35%|███▍      | 14000/40430 [00:04<00:07, 3579.45 examples/s]Map:  37%|███▋      | 15000/40430 [00:04<00:06, 3668.26 examples/s]Map:  40%|███▉      | 16000/40430 [00:04<00:06, 3723.85 examples/s]Map:  42%|████▏     | 17000/40430 [00:04<00:06, 3815.41 examples/s]Map:  45%|████▍     | 18000/40430 [00:05<00:05, 3851.55 examples/s]Map:  47%|████▋     | 19000/40430 [00:05<00:05, 3886.97 examples/s]Map:  49%|████▉     | 20000/40430 [00:05<00:05, 3901.75 examples/s]Map:  52%|█████▏    | 21000/40430 [00:06<00:05, 3433.45 examples/s]Map:  54%|█████▍    | 22000/40430 [00:06<00:05, 3572.14 examples/s]Map:  57%|█████▋    | 23000/40430 [00:06<00:04, 3597.54 examples/s]Map:  59%|█████▉    | 24000/40430 [00:06<00:04, 3657.33 examples/s]Map:  62%|██████▏   | 25000/40430 [00:07<00:04, 3696.28 examples/s]Map:  64%|██████▍   | 26000/40430 [00:07<00:03, 3756.18 examples/s]Map:  67%|██████▋   | 27000/40430 [00:07<00:03, 3801.98 examples/s]Map:  69%|██████▉   | 28000/40430 [00:07<00:03, 3826.85 examples/s]Map:  72%|███████▏  | 29000/40430 [00:08<00:02, 3872.97 examples/s]Map:  74%|███████▍  | 30000/40430 [00:08<00:02, 3924.82 examples/s]Map:  77%|███████▋  | 31000/40430 [00:08<00:02, 3940.27 examples/s]Map:  79%|███████▉  | 32000/40430 [00:08<00:02, 3932.59 examples/s]Map:  82%|████████▏ | 33000/40430 [00:09<00:01, 3921.34 examples/s]Map:  84%|████████▍ | 34000/40430 [00:09<00:01, 3928.59 examples/s]Map:  87%|████████▋ | 35000/40430 [00:09<00:01, 3435.53 examples/s]Map:  89%|████████▉ | 36000/40430 [00:10<00:01, 3555.29 examples/s]Map:  92%|█████████▏| 37000/40430 [00:10<00:00, 3653.86 examples/s]Map:  94%|█████████▍| 38000/40430 [00:10<00:00, 3713.17 examples/s]Map:  96%|█████████▋| 39000/40430 [00:10<00:00, 3776.66 examples/s]Map:  99%|█████████▉| 40000/40430 [00:11<00:00, 3794.08 examples/s]Map: 100%|██████████| 40430/40430 [00:11<00:00, 3753.22 examples/s]                                                                   slurmstepd-gypsum-gpu066: error: *** JOB 7680865 ON gypsum-gpu066 CANCELLED AT 2023-06-04T18:14:56 ***
