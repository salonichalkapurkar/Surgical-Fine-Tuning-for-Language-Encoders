######################################################################
full model fine-tuning
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 377.03it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4916: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.
  warnings.warn(stats.ConstantInputWarning(warn_msg))
{'eval_loss': 3.1661012172698975, 'eval_spearmanr': nan, 'eval_runtime': 52.8622, 'eval_samples_per_second': 28.376, 'eval_steps_per_second': 3.556, 'epoch': 1.0}
{'loss': 4.4041, 'learning_rate': 2.5e-06, 'epoch': 1.39}
{'eval_loss': 0.6740052700042725, 'eval_spearmanr': nan, 'eval_runtime': 52.8162, 'eval_samples_per_second': 28.4, 'eval_steps_per_second': 3.56, 'epoch': 2.0}
{'loss': 0.7535, 'learning_rate': 5e-06, 'epoch': 2.78}
{'eval_loss': 0.589287519454956, 'eval_spearmanr': nan, 'eval_runtime': 52.798, 'eval_samples_per_second': 28.41, 'eval_steps_per_second': 3.561, 'epoch': 3.0}
{'eval_loss': 0.627828061580658, 'eval_spearmanr': nan, 'eval_runtime': 52.7962, 'eval_samples_per_second': 28.411, 'eval_steps_per_second': 3.561, 'epoch': 4.0}
{'loss': 0.4963, 'learning_rate': 7.5e-06, 'epoch': 4.17}
{'eval_loss': 0.4673771262168884, 'eval_spearmanr': nan, 'eval_runtime': 52.7813, 'eval_samples_per_second': 28.419, 'eval_steps_per_second': 3.562, 'epoch': 5.0}
{'loss': 0.3246, 'learning_rate': 1e-05, 'epoch': 5.56}
{'eval_loss': 0.47118425369262695, 'eval_spearmanr': nan, 'eval_runtime': 52.8096, 'eval_samples_per_second': 28.404, 'eval_steps_per_second': 3.56, 'epoch': 6.0}
{'loss': 0.2271, 'learning_rate': 1.25e-05, 'epoch': 6.95}
{'eval_loss': 0.5003071427345276, 'eval_spearmanr': nan, 'eval_runtime': 52.8345, 'eval_samples_per_second': 28.391, 'eval_steps_per_second': 3.558, 'epoch': 7.0}
{'eval_loss': 0.47661709785461426, 'eval_spearmanr': nan, 'eval_runtime': 52.8316, 'eval_samples_per_second': 28.392, 'eval_steps_per_second': 3.558, 'epoch': 8.0}
{'loss': 0.1612, 'learning_rate': 1.5e-05, 'epoch': 8.34}
{'eval_loss': 0.4796113967895508, 'eval_spearmanr': nan, 'eval_runtime': 52.8512, 'eval_samples_per_second': 28.382, 'eval_steps_per_second': 3.557, 'epoch': 9.0}
{'loss': 0.1404, 'learning_rate': 1.75e-05, 'epoch': 9.74}
{'eval_loss': 0.48669376969337463, 'eval_spearmanr': nan, 'eval_runtime': 52.8332, 'eval_samples_per_second': 28.391, 'eval_steps_per_second': 3.558, 'epoch': 9.99}
{'train_runtime': 6176.5581, 'train_samples_per_second': 9.308, 'train_steps_per_second': 0.581, 'train_loss': 0.9093450110602844, 'epoch': 9.99}
Total training time: 6176.57 seconds
######################################################################
layer-wise fine-tuning top 1
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 398.07it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4916: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.
  warnings.warn(stats.ConstantInputWarning(warn_msg))
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.1', 'bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.4', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 3.8619022369384766, 'eval_spearmanr': nan, 'eval_runtime': 52.9262, 'eval_samples_per_second': 28.341, 'eval_steps_per_second': 3.552, 'epoch': 1.0}
{'loss': 5.3295, 'learning_rate': 2.5e-06, 'epoch': 1.39}
{'eval_loss': 1.8209222555160522, 'eval_spearmanr': nan, 'eval_runtime': 52.9368, 'eval_samples_per_second': 28.336, 'eval_steps_per_second': 3.551, 'epoch': 2.0}
{'loss': 2.1663, 'learning_rate': 5e-06, 'epoch': 2.78}
{'eval_loss': 0.919974684715271, 'eval_spearmanr': nan, 'eval_runtime': 52.8743, 'eval_samples_per_second': 28.369, 'eval_steps_per_second': 3.556, 'epoch': 3.0}
{'eval_loss': 0.7458701133728027, 'eval_spearmanr': nan, 'eval_runtime': 52.8479, 'eval_samples_per_second': 28.383, 'eval_steps_per_second': 3.557, 'epoch': 4.0}
{'loss': 0.9568, 'learning_rate': 7.5e-06, 'epoch': 4.17}
{'eval_loss': 0.6588722467422485, 'eval_spearmanr': nan, 'eval_runtime': 52.8637, 'eval_samples_per_second': 28.375, 'eval_steps_per_second': 3.556, 'epoch': 5.0}
{'loss': 0.745, 'learning_rate': 1e-05, 'epoch': 5.56}
{'eval_loss': 0.6037446856498718, 'eval_spearmanr': nan, 'eval_runtime': 52.8928, 'eval_samples_per_second': 28.359, 'eval_steps_per_second': 3.554, 'epoch': 6.0}
{'loss': 0.5941, 'learning_rate': 1.25e-05, 'epoch': 6.95}
{'eval_loss': 0.5895807147026062, 'eval_spearmanr': nan, 'eval_runtime': 52.837, 'eval_samples_per_second': 28.389, 'eval_steps_per_second': 3.558, 'epoch': 7.0}
{'eval_loss': 0.5714678764343262, 'eval_spearmanr': nan, 'eval_runtime': 52.8451, 'eval_samples_per_second': 28.385, 'eval_steps_per_second': 3.558, 'epoch': 8.0}
{'loss': 0.4614, 'learning_rate': 1.5e-05, 'epoch': 8.34}
{'eval_loss': 0.5584475994110107, 'eval_spearmanr': nan, 'eval_runtime': 52.8358, 'eval_samples_per_second': 28.39, 'eval_steps_per_second': 3.558, 'epoch': 9.0}
{'loss': 0.3772, 'learning_rate': 1.75e-05, 'epoch': 9.74}
{'eval_loss': 0.5697927474975586, 'eval_spearmanr': nan, 'eval_runtime': 52.9157, 'eval_samples_per_second': 28.347, 'eval_steps_per_second': 3.553, 'epoch': 9.99}
{'train_runtime': 4901.3424, 'train_samples_per_second': 11.729, 'train_steps_per_second': 0.732, 'train_loss': 1.4891382812457497, 'epoch': 9.99}
Total training time: 4901.36 seconds
######################################################################
layer-wise fine-tuning top 2
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 407.36it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4916: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.
  warnings.warn(stats.ConstantInputWarning(warn_msg))
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.4', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 3.855280637741089, 'eval_spearmanr': nan, 'eval_runtime': 52.7317, 'eval_samples_per_second': 28.446, 'eval_steps_per_second': 3.565, 'epoch': 1.0}
{'loss': 5.3215, 'learning_rate': 2.5e-06, 'epoch': 1.39}
{'eval_loss': 1.8052700757980347, 'eval_spearmanr': nan, 'eval_runtime': 52.7382, 'eval_samples_per_second': 28.442, 'eval_steps_per_second': 3.565, 'epoch': 2.0}
{'loss': 2.1418, 'learning_rate': 5e-06, 'epoch': 2.78}
{'eval_loss': 0.8970844745635986, 'eval_spearmanr': nan, 'eval_runtime': 52.7305, 'eval_samples_per_second': 28.447, 'eval_steps_per_second': 3.565, 'epoch': 3.0}
{'eval_loss': 0.6973071098327637, 'eval_spearmanr': nan, 'eval_runtime': 52.7355, 'eval_samples_per_second': 28.444, 'eval_steps_per_second': 3.565, 'epoch': 4.0}
{'loss': 0.8987, 'learning_rate': 7.5e-06, 'epoch': 4.17}
{'eval_loss': 0.6249801516532898, 'eval_spearmanr': nan, 'eval_runtime': 52.715, 'eval_samples_per_second': 28.455, 'eval_steps_per_second': 3.566, 'epoch': 5.0}
{'loss': 0.6676, 'learning_rate': 1e-05, 'epoch': 5.56}
{'eval_loss': 0.5904811024665833, 'eval_spearmanr': nan, 'eval_runtime': 52.6935, 'eval_samples_per_second': 28.466, 'eval_steps_per_second': 3.568, 'epoch': 6.0}
{'loss': 0.5163, 'learning_rate': 1.25e-05, 'epoch': 6.95}
{'eval_loss': 0.5707074403762817, 'eval_spearmanr': nan, 'eval_runtime': 52.7199, 'eval_samples_per_second': 28.452, 'eval_steps_per_second': 3.566, 'epoch': 7.0}
{'eval_loss': 0.5685792565345764, 'eval_spearmanr': nan, 'eval_runtime': 52.7222, 'eval_samples_per_second': 28.451, 'eval_steps_per_second': 3.566, 'epoch': 8.0}
{'loss': 0.3792, 'learning_rate': 1.5e-05, 'epoch': 8.34}
{'eval_loss': 0.563836932182312, 'eval_spearmanr': nan, 'eval_runtime': 52.7219, 'eval_samples_per_second': 28.451, 'eval_steps_per_second': 3.566, 'epoch': 9.0}
{'loss': 0.2986, 'learning_rate': 1.75e-05, 'epoch': 9.74}
{'eval_loss': 0.5633239150047302, 'eval_spearmanr': nan, 'eval_runtime': 52.7432, 'eval_samples_per_second': 28.44, 'eval_steps_per_second': 3.564, 'epoch': 9.99}
{'train_runtime': 5015.7559, 'train_samples_per_second': 11.462, 'train_steps_per_second': 0.716, 'train_loss': 1.430503986007988, 'epoch': 9.99}
Total training time: 5015.77 seconds
######################################################################
layer-wise fine-tuning top 3
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 443.22it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4916: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.
  warnings.warn(stats.ConstantInputWarning(warn_msg))
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 3.833824634552002, 'eval_spearmanr': nan, 'eval_runtime': 52.8484, 'eval_samples_per_second': 28.383, 'eval_steps_per_second': 3.557, 'epoch': 1.0}
{'loss': 5.3007, 'learning_rate': 2.5e-06, 'epoch': 1.39}
{'eval_loss': 1.7819161415100098, 'eval_spearmanr': nan, 'eval_runtime': 52.8031, 'eval_samples_per_second': 28.407, 'eval_steps_per_second': 3.56, 'epoch': 2.0}
{'loss': 2.0945, 'learning_rate': 5e-06, 'epoch': 2.78}
{'eval_loss': 0.7427722215652466, 'eval_spearmanr': nan, 'eval_runtime': 52.9702, 'eval_samples_per_second': 28.318, 'eval_steps_per_second': 3.549, 'epoch': 3.0}
{'eval_loss': 0.6031172871589661, 'eval_spearmanr': nan, 'eval_runtime': 52.7838, 'eval_samples_per_second': 28.418, 'eval_steps_per_second': 3.562, 'epoch': 4.0}
{'loss': 0.7747, 'learning_rate': 7.5e-06, 'epoch': 4.17}
{'eval_loss': 0.5570763349533081, 'eval_spearmanr': nan, 'eval_runtime': 52.8136, 'eval_samples_per_second': 28.402, 'eval_steps_per_second': 3.56, 'epoch': 5.0}
{'loss': 0.5463, 'learning_rate': 1e-05, 'epoch': 5.56}
{'eval_loss': 0.555627703666687, 'eval_spearmanr': nan, 'eval_runtime': 52.814, 'eval_samples_per_second': 28.402, 'eval_steps_per_second': 3.56, 'epoch': 6.0}
{'loss': 0.413, 'learning_rate': 1.25e-05, 'epoch': 6.95}
{'eval_loss': 0.5348935723304749, 'eval_spearmanr': nan, 'eval_runtime': 52.8385, 'eval_samples_per_second': 28.388, 'eval_steps_per_second': 3.558, 'epoch': 7.0}
{'eval_loss': 0.5552850961685181, 'eval_spearmanr': nan, 'eval_runtime': 52.7801, 'eval_samples_per_second': 28.42, 'eval_steps_per_second': 3.562, 'epoch': 8.0}
{'loss': 0.2987, 'learning_rate': 1.5e-05, 'epoch': 8.34}
{'eval_loss': 0.5585810542106628, 'eval_spearmanr': nan, 'eval_runtime': 52.8423, 'eval_samples_per_second': 28.386, 'eval_steps_per_second': 3.558, 'epoch': 9.0}
{'loss': 0.2323, 'learning_rate': 1.75e-05, 'epoch': 9.74}
{'eval_loss': 0.5304022431373596, 'eval_spearmanr': nan, 'eval_runtime': 52.85, 'eval_samples_per_second': 28.382, 'eval_steps_per_second': 3.557, 'epoch': 9.99}
{'train_runtime': 5143.9304, 'train_samples_per_second': 11.176, 'train_steps_per_second': 0.698, 'train_loss': 1.3503720705887734, 'epoch': 9.99}
Total training time: 5143.94 seconds
######################################################################
layer-wise fine-tuning top 4
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 336.77it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4916: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.
  warnings.warn(stats.ConstantInputWarning(warn_msg))
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.2', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 3.8200502395629883, 'eval_spearmanr': nan, 'eval_runtime': 52.9522, 'eval_samples_per_second': 28.327, 'eval_steps_per_second': 3.55, 'epoch': 1.0}
{'loss': 5.2868, 'learning_rate': 2.5e-06, 'epoch': 1.39}
{'eval_loss': 1.7503536939620972, 'eval_spearmanr': nan, 'eval_runtime': 52.8977, 'eval_samples_per_second': 28.357, 'eval_steps_per_second': 3.554, 'epoch': 2.0}
{'loss': 2.0032, 'learning_rate': 5e-06, 'epoch': 2.78}
{'eval_loss': 0.626040518283844, 'eval_spearmanr': nan, 'eval_runtime': 52.936, 'eval_samples_per_second': 28.336, 'eval_steps_per_second': 3.551, 'epoch': 3.0}
{'eval_loss': 0.5762860774993896, 'eval_spearmanr': nan, 'eval_runtime': 52.8195, 'eval_samples_per_second': 28.399, 'eval_steps_per_second': 3.559, 'epoch': 4.0}
{'loss': 0.6916, 'learning_rate': 7.5e-06, 'epoch': 4.17}
{'eval_loss': 0.5219480991363525, 'eval_spearmanr': nan, 'eval_runtime': 52.8979, 'eval_samples_per_second': 28.357, 'eval_steps_per_second': 3.554, 'epoch': 5.0}
{'loss': 0.4932, 'learning_rate': 1e-05, 'epoch': 5.56}
{'eval_loss': 0.518719494342804, 'eval_spearmanr': nan, 'eval_runtime': 52.837, 'eval_samples_per_second': 28.389, 'eval_steps_per_second': 3.558, 'epoch': 6.0}
{'loss': 0.369, 'learning_rate': 1.25e-05, 'epoch': 6.95}
{'eval_loss': 0.5169006586074829, 'eval_spearmanr': nan, 'eval_runtime': 52.9783, 'eval_samples_per_second': 28.313, 'eval_steps_per_second': 3.549, 'epoch': 7.0}
{'eval_loss': 0.5454881191253662, 'eval_spearmanr': nan, 'eval_runtime': 52.897, 'eval_samples_per_second': 28.357, 'eval_steps_per_second': 3.554, 'epoch': 8.0}
{'loss': 0.275, 'learning_rate': 1.5e-05, 'epoch': 8.34}
{'eval_loss': 0.5121510624885559, 'eval_spearmanr': nan, 'eval_runtime': 52.8781, 'eval_samples_per_second': 28.367, 'eval_steps_per_second': 3.555, 'epoch': 9.0}
{'loss': 0.214, 'learning_rate': 1.75e-05, 'epoch': 9.74}
{'eval_loss': 0.516548752784729, 'eval_spearmanr': nan, 'eval_runtime': 52.8487, 'eval_samples_per_second': 28.383, 'eval_steps_per_second': 3.557, 'epoch': 9.99}
{'train_runtime': 5251.92, 'train_samples_per_second': 10.946, 'train_steps_per_second': 0.684, 'train_loss': 1.304433749313142, 'epoch': 9.99}
Total training time: 5251.93 seconds
######################################################################
layer-wise fine-tuning top 5
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 369.48it/s]
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/schalkapurka_umass_edu/.local/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4916: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.
  warnings.warn(stats.ConstantInputWarning(warn_msg))
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 3.810004472732544, 'eval_spearmanr': nan, 'eval_runtime': 52.8493, 'eval_samples_per_second': 28.383, 'eval_steps_per_second': 3.557, 'epoch': 1.0}
{'loss': 5.2779, 'learning_rate': 2.5e-06, 'epoch': 1.39}
{'eval_loss': 1.5413442850112915, 'eval_spearmanr': nan, 'eval_runtime': 52.8279, 'eval_samples_per_second': 28.394, 'eval_steps_per_second': 3.559, 'epoch': 2.0}
{'loss': 1.8719, 'learning_rate': 5e-06, 'epoch': 2.78}
{'eval_loss': 0.6405481100082397, 'eval_spearmanr': nan, 'eval_runtime': 52.8986, 'eval_samples_per_second': 28.356, 'eval_steps_per_second': 3.554, 'epoch': 3.0}
{'eval_loss': 0.6285267472267151, 'eval_spearmanr': nan, 'eval_runtime': 52.864, 'eval_samples_per_second': 28.375, 'eval_steps_per_second': 3.556, 'epoch': 4.0}
{'loss': 0.6752, 'learning_rate': 7.5e-06, 'epoch': 4.17}
{'eval_loss': 0.6299600005149841, 'eval_spearmanr': nan, 'eval_runtime': 52.8178, 'eval_samples_per_second': 28.4, 'eval_steps_per_second': 3.559, 'epoch': 5.0}
{'loss': 0.4762, 'learning_rate': 1e-05, 'epoch': 5.56}
{'eval_loss': 0.5518088936805725, 'eval_spearmanr': nan, 'eval_runtime': 52.8186, 'eval_samples_per_second': 28.399, 'eval_steps_per_second': 3.559, 'epoch': 6.0}
{'loss': 0.3541, 'learning_rate': 1.25e-05, 'epoch': 6.95}
{'eval_loss': 0.5810202360153198, 'eval_spearmanr': nan, 'eval_runtime': 52.8576, 'eval_samples_per_second': 28.378, 'eval_steps_per_second': 3.557, 'epoch': 7.0}
{'eval_loss': 0.5988354682922363, 'eval_spearmanr': nan, 'eval_runtime': 52.9122, 'eval_samples_per_second': 28.349, 'eval_steps_per_second': 3.553, 'epoch': 8.0}
{'loss': 0.2593, 'learning_rate': 1.5e-05, 'epoch': 8.34}
{'eval_loss': 0.5693926811218262, 'eval_spearmanr': nan, 'eval_runtime': 52.8467, 'eval_samples_per_second': 28.384, 'eval_steps_per_second': 3.557, 'epoch': 9.0}
{'loss': 0.2, 'learning_rate': 1.75e-05, 'epoch': 9.74}
{'eval_loss': 0.5610345005989075, 'eval_spearmanr': nan, 'eval_runtime': 52.8955, 'eval_samples_per_second': 28.358, 'eval_steps_per_second': 3.554, 'epoch': 9.99}
{'train_runtime': 5370.8663, 'train_samples_per_second': 10.704, 'train_steps_per_second': 0.668, 'train_loss': 1.2740547775226052, 'epoch': 9.99}
Total training time: 5370.88 seconds
######################################################################
finished
