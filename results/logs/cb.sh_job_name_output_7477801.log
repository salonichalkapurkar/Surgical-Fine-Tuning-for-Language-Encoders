Loading miniconda version 22.11.1-1
######################################################################
full model fine-tuning
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 170.77it/s]
/home/yuanmingtao_umass_edu/.conda/envs/petl/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'eval_loss': 1.0624347925186157, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7253, 'eval_samples_per_second': 32.458, 'eval_steps_per_second': 2.318, 'epoch': 1.0}
{'eval_loss': 1.0622481107711792, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7479, 'eval_samples_per_second': 32.039, 'eval_steps_per_second': 2.288, 'epoch': 2.0}
{'eval_loss': 1.06192946434021, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7395, 'eval_samples_per_second': 32.193, 'eval_steps_per_second': 2.3, 'epoch': 3.0}
{'eval_loss': 1.0615004301071167, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7204, 'eval_samples_per_second': 32.55, 'eval_steps_per_second': 2.325, 'epoch': 4.0}
{'eval_loss': 1.0609623193740845, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7386, 'eval_samples_per_second': 32.21, 'eval_steps_per_second': 2.301, 'epoch': 5.0}
{'eval_loss': 1.0603313446044922, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7226, 'eval_samples_per_second': 32.51, 'eval_steps_per_second': 2.322, 'epoch': 6.0}
{'eval_loss': 1.0596206188201904, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.747, 'eval_samples_per_second': 32.055, 'eval_steps_per_second': 2.29, 'epoch': 7.0}
{'eval_loss': 1.0588489770889282, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7235, 'eval_samples_per_second': 32.492, 'eval_steps_per_second': 2.321, 'epoch': 8.0}
{'eval_loss': 1.0579661130905151, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7281, 'eval_samples_per_second': 32.406, 'eval_steps_per_second': 2.315, 'epoch': 9.0}
{'eval_loss': 1.056908369064331, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7238, 'eval_samples_per_second': 32.487, 'eval_steps_per_second': 2.321, 'epoch': 10.0}
{'train_runtime': 241.1841, 'train_samples_per_second': 10.366, 'train_steps_per_second': 0.332, 'train_loss': 1.0263304710388184, 'epoch': 10.0}
Total training time: 241.20 seconds
######################################################################
layer-wise fine-tuning top 1
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 343.23it/s]
/home/yuanmingtao_umass_edu/.conda/envs/petl/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.1', 'bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.4', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 1.0624676942825317, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7391, 'eval_samples_per_second': 32.2, 'eval_steps_per_second': 2.3, 'epoch': 1.0}
{'eval_loss': 1.0623986721038818, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7243, 'eval_samples_per_second': 32.478, 'eval_steps_per_second': 2.32, 'epoch': 2.0}
{'eval_loss': 1.0622830390930176, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.731, 'eval_samples_per_second': 32.352, 'eval_steps_per_second': 2.311, 'epoch': 3.0}
{'eval_loss': 1.0621240139007568, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.732, 'eval_samples_per_second': 32.332, 'eval_steps_per_second': 2.309, 'epoch': 4.0}
{'eval_loss': 1.0619217157363892, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7201, 'eval_samples_per_second': 32.556, 'eval_steps_per_second': 2.325, 'epoch': 5.0}
{'eval_loss': 1.061678171157837, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7213, 'eval_samples_per_second': 32.534, 'eval_steps_per_second': 2.324, 'epoch': 6.0}
{'eval_loss': 1.0614041090011597, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7331, 'eval_samples_per_second': 32.312, 'eval_steps_per_second': 2.308, 'epoch': 7.0}
{'eval_loss': 1.0610814094543457, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7392, 'eval_samples_per_second': 32.198, 'eval_steps_per_second': 2.3, 'epoch': 8.0}
{'eval_loss': 1.0607012510299683, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7365, 'eval_samples_per_second': 32.249, 'eval_steps_per_second': 2.304, 'epoch': 9.0}
{'eval_loss': 1.0602686405181885, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7238, 'eval_samples_per_second': 32.486, 'eval_steps_per_second': 2.32, 'epoch': 10.0}
{'train_runtime': 193.2121, 'train_samples_per_second': 12.939, 'train_steps_per_second': 0.414, 'train_loss': 1.0278188705444335, 'epoch': 10.0}
Total training time: 193.23 seconds
######################################################################
layer-wise fine-tuning top 2
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 382.12it/s]
/home/yuanmingtao_umass_edu/.conda/envs/petl/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.4', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 1.062467336654663, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7661, 'eval_samples_per_second': 31.709, 'eval_steps_per_second': 2.265, 'epoch': 1.0}
{'eval_loss': 1.0623964071273804, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7225, 'eval_samples_per_second': 32.51, 'eval_steps_per_second': 2.322, 'epoch': 2.0}
{'eval_loss': 1.0622775554656982, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7201, 'eval_samples_per_second': 32.557, 'eval_steps_per_second': 2.326, 'epoch': 3.0}
{'eval_loss': 1.062113881111145, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7288, 'eval_samples_per_second': 32.392, 'eval_steps_per_second': 2.314, 'epoch': 4.0}
{'eval_loss': 1.0619064569473267, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7251, 'eval_samples_per_second': 32.462, 'eval_steps_per_second': 2.319, 'epoch': 5.0}
{'eval_loss': 1.061657428741455, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7193, 'eval_samples_per_second': 32.572, 'eval_steps_per_second': 2.327, 'epoch': 6.0}
{'eval_loss': 1.0613775253295898, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7289, 'eval_samples_per_second': 32.391, 'eval_steps_per_second': 2.314, 'epoch': 7.0}
{'eval_loss': 1.0610487461090088, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7352, 'eval_samples_per_second': 32.274, 'eval_steps_per_second': 2.305, 'epoch': 8.0}
{'eval_loss': 1.060662865638733, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7936, 'eval_samples_per_second': 31.222, 'eval_steps_per_second': 2.23, 'epoch': 9.0}
{'eval_loss': 1.0602222681045532, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.765, 'eval_samples_per_second': 31.729, 'eval_steps_per_second': 2.266, 'epoch': 10.0}
{'train_runtime': 197.5545, 'train_samples_per_second': 12.655, 'train_steps_per_second': 0.405, 'train_loss': 1.0277756690979003, 'epoch': 10.0}
Total training time: 197.57 seconds
######################################################################
layer-wise fine-tuning top 3
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 394.71it/s]
/home/yuanmingtao_umass_edu/.conda/envs/petl/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.2', 'bert.encoder.layer.3', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 1.062465786933899, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7287, 'eval_samples_per_second': 32.394, 'eval_steps_per_second': 2.314, 'epoch': 1.0}
{'eval_loss': 1.0623881816864014, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7505, 'eval_samples_per_second': 31.991, 'eval_steps_per_second': 2.285, 'epoch': 2.0}
{'eval_loss': 1.0622575283050537, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7388, 'eval_samples_per_second': 32.206, 'eval_steps_per_second': 2.3, 'epoch': 3.0}
{'eval_loss': 1.0620790719985962, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7188, 'eval_samples_per_second': 32.581, 'eval_steps_per_second': 2.327, 'epoch': 4.0}
{'eval_loss': 1.0618531703948975, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.72, 'eval_samples_per_second': 32.557, 'eval_steps_per_second': 2.326, 'epoch': 5.0}
{'eval_loss': 1.0615835189819336, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7333, 'eval_samples_per_second': 32.308, 'eval_steps_per_second': 2.308, 'epoch': 6.0}
{'eval_loss': 1.0612794160842896, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7264, 'eval_samples_per_second': 32.437, 'eval_steps_per_second': 2.317, 'epoch': 7.0}
{'eval_loss': 1.0609281063079834, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7275, 'eval_samples_per_second': 32.417, 'eval_steps_per_second': 2.316, 'epoch': 8.0}
{'eval_loss': 1.060518503189087, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7713, 'eval_samples_per_second': 31.615, 'eval_steps_per_second': 2.258, 'epoch': 9.0}
{'eval_loss': 1.0600457191467285, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7276, 'eval_samples_per_second': 32.414, 'eval_steps_per_second': 2.315, 'epoch': 10.0}
{'train_runtime': 202.052, 'train_samples_per_second': 12.373, 'train_steps_per_second': 0.396, 'train_loss': 1.0276898384094237, 'epoch': 10.0}
Total training time: 202.08 seconds
######################################################################
layer-wise fine-tuning top 4
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 353.53it/s]
/home/yuanmingtao_umass_edu/.conda/envs/petl/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.2', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 1.0624643564224243, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7325, 'eval_samples_per_second': 32.323, 'eval_steps_per_second': 2.309, 'epoch': 1.0}
{'eval_loss': 1.062382459640503, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7345, 'eval_samples_per_second': 32.286, 'eval_steps_per_second': 2.306, 'epoch': 2.0}
{'eval_loss': 1.062243938446045, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7278, 'eval_samples_per_second': 32.411, 'eval_steps_per_second': 2.315, 'epoch': 3.0}
{'eval_loss': 1.0620554685592651, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7211, 'eval_samples_per_second': 32.538, 'eval_steps_per_second': 2.324, 'epoch': 4.0}
{'eval_loss': 1.0618174076080322, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7296, 'eval_samples_per_second': 32.377, 'eval_steps_per_second': 2.313, 'epoch': 5.0}
{'eval_loss': 1.0615342855453491, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7195, 'eval_samples_per_second': 32.568, 'eval_steps_per_second': 2.326, 'epoch': 6.0}
{'eval_loss': 1.0612143278121948, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7211, 'eval_samples_per_second': 32.537, 'eval_steps_per_second': 2.324, 'epoch': 7.0}
{'eval_loss': 1.0608489513397217, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7476, 'eval_samples_per_second': 32.044, 'eval_steps_per_second': 2.289, 'epoch': 8.0}
{'eval_loss': 1.0604240894317627, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7585, 'eval_samples_per_second': 31.845, 'eval_steps_per_second': 2.275, 'epoch': 9.0}
{'eval_loss': 1.059930443763733, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7165, 'eval_samples_per_second': 32.625, 'eval_steps_per_second': 2.33, 'epoch': 10.0}
{'train_runtime': 206.1767, 'train_samples_per_second': 12.126, 'train_steps_per_second': 0.388, 'train_loss': 1.0276206016540528, 'epoch': 10.0}
Total training time: 206.19 seconds
######################################################################
layer-wise fine-tuning top 5
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 347.58it/s]
/home/yuanmingtao_umass_edu/.conda/envs/petl/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
layers to freeze ('bert.encoder.layer.0', 'bert.encoder.layer.6', 'bert.encoder.layer.7', 'bert.encoder.layer.8', 'bert.encoder.layer.9', 'bert.encoder.layer.10', 'bert.encoder.layer.11')
{'eval_loss': 1.0624637603759766, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7239, 'eval_samples_per_second': 32.485, 'eval_steps_per_second': 2.32, 'epoch': 1.0}
{'eval_loss': 1.0623785257339478, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7178, 'eval_samples_per_second': 32.599, 'eval_steps_per_second': 2.329, 'epoch': 2.0}
{'eval_loss': 1.0622345209121704, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7691, 'eval_samples_per_second': 31.655, 'eval_steps_per_second': 2.261, 'epoch': 3.0}
{'eval_loss': 1.062038779258728, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7235, 'eval_samples_per_second': 32.492, 'eval_steps_per_second': 2.321, 'epoch': 4.0}
{'eval_loss': 1.0617924928665161, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7162, 'eval_samples_per_second': 32.63, 'eval_steps_per_second': 2.331, 'epoch': 5.0}
{'eval_loss': 1.0614999532699585, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7273, 'eval_samples_per_second': 32.421, 'eval_steps_per_second': 2.316, 'epoch': 6.0}
{'eval_loss': 1.0611697435379028, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7276, 'eval_samples_per_second': 32.415, 'eval_steps_per_second': 2.315, 'epoch': 7.0}
{'eval_loss': 1.0607943534851074, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7368, 'eval_samples_per_second': 32.243, 'eval_steps_per_second': 2.303, 'epoch': 8.0}
{'eval_loss': 1.0603595972061157, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7312, 'eval_samples_per_second': 32.348, 'eval_steps_per_second': 2.311, 'epoch': 9.0}
{'eval_loss': 1.0598520040512085, 'eval_accuracy': 0.39285714285714285, 'eval_runtime': 1.7215, 'eval_samples_per_second': 32.53, 'eval_steps_per_second': 2.324, 'epoch': 10.0}
{'train_runtime': 210.5286, 'train_samples_per_second': 11.875, 'train_steps_per_second': 0.38, 'train_loss': 1.027565860748291, 'epoch': 10.0}
Total training time: 210.54 seconds
######################################################################
finished
