######################################################################
full model fine-tuning
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Using cuda
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:00<00:00,  6.41it/s]100%|██████████| 3/3 [00:00<00:00, 17.55it/s]
/home/gbelapurkar_umass_edu/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.6888, 'learning_rate': 2.5e-06, 'epoch': 0.12}
{'loss': 0.3503, 'learning_rate': 5e-06, 'epoch': 0.24}
{'loss': 0.2874, 'learning_rate': 7.5e-06, 'epoch': 0.36}
{'loss': 0.2602, 'learning_rate': 1e-05, 'epoch': 0.48}
{'loss': 0.2401, 'learning_rate': 1.25e-05, 'epoch': 0.59}
{'loss': 0.2207, 'learning_rate': 1.5e-05, 'epoch': 0.71}
{'loss': 0.2285, 'learning_rate': 1.75e-05, 'epoch': 0.83}
{'loss': 0.217, 'learning_rate': 2e-05, 'epoch': 0.95}
{'eval_loss': 0.2623204290866852, 'eval_accuracy': 0.9174311926605505, 'eval_runtime': 27.2746, 'eval_samples_per_second': 31.971, 'eval_steps_per_second': 3.996, 'epoch': 1.0}
{'loss': 0.1935, 'learning_rate': 2.25e-05, 'epoch': 1.07}
{'loss': 0.193, 'learning_rate': 2.5e-05, 'epoch': 1.19}
{'loss': 0.1952, 'learning_rate': 2.7500000000000004e-05, 'epoch': 1.31}
{'loss': 0.1878, 'learning_rate': 3e-05, 'epoch': 1.43}
{'loss': 0.1857, 'learning_rate': 3.2500000000000004e-05, 'epoch': 1.54}
{'loss': 0.2062, 'learning_rate': 3.5e-05, 'epoch': 1.66}
{'loss': 0.2037, 'learning_rate': 3.7500000000000003e-05, 'epoch': 1.78}
{'loss': 0.1995, 'learning_rate': 4e-05, 'epoch': 1.9}
{'eval_loss': 0.2818411588668823, 'eval_accuracy': 0.9128440366972477, 'eval_runtime': 27.2919, 'eval_samples_per_second': 31.951, 'eval_steps_per_second': 3.994, 'epoch': 2.0}
{'loss': 0.1844, 'learning_rate': 4.25e-05, 'epoch': 2.02}
{'loss': 0.1593, 'learning_rate': 4.5e-05, 'epoch': 2.14}
{'loss': 0.1696, 'learning_rate': 4.75e-05, 'epoch': 2.26}
{'loss': 0.1587, 'learning_rate': 5e-05, 'epoch': 2.38}
{'loss': 0.1683, 'learning_rate': 4.0483441187666544e-05, 'epoch': 2.49}
{'loss': 0.1559, 'learning_rate': 3.096688237533308e-05, 'epoch': 2.61}
{'loss': 0.156, 'learning_rate': 2.1450323562999618e-05, 'epoch': 2.73}
{'loss': 0.1416, 'learning_rate': 1.193376475066616e-05, 'epoch': 2.85}
{'loss': 0.1357, 'learning_rate': 2.417205938332699e-06, 'epoch': 2.97}
{'eval_loss': 0.3223972022533417, 'eval_accuracy': 0.9071100917431193, 'eval_runtime': 27.2749, 'eval_samples_per_second': 31.971, 'eval_steps_per_second': 3.996, 'epoch': 3.0}
{'train_runtime': 19339.8301, 'train_samples_per_second': 10.447, 'train_steps_per_second': 0.653, 'train_loss': 0.21853990374301008, 'epoch': 3.0}
Total training time: 19339.86 seconds
